In the previous chapter, we have learned how compilation to high-level targets can present an easy way of implementing a compiler.
These compilers generated outputs which were still platform independent and portable.
However, compilers can also target a specific computer architecture directly, thus removing another layer of abstraction.
The concept of compiling to a specific architecture directly is similar to the VM since its compiler also targets its architecture directly.
However, implementing a compiler targeting the architectures presented in this chapter has proven to be a lot more demanding since the VM uses a fictional architecture purposefully developed for this paper.
Reasons for this chapter's difficulty mostly include target-specific constraints which were
still irrelevant in the previous chapter.
This chapter contains the term \enquote{low-level} in its name since the presented compilers
generate code for specific target-architectures directly.

\section{Low-Level Programming Concepts}
Programming using high-level languages does not require knowledge about the intrinsic of the target architecture.
However, in this chapter, two compilers targeting the low-level assembly language are presented.
In order to make sections in which these compilers are explained more approachable,
some of the most important low-level programming concepts are explained in this section.
We will only explain concepts which play a significant role later on.

% \begin{itemize}
%     \item Basic structure of an ELF assembly program (use RISC-V as a simple example)~\cite[p.~35]{Patterson2017}
%     \item Registers \& why their allocation is a problem (with figure describing CPU, registers, Memory)~\cite[p.~10]{Patterson2017}
%     \item Assembly \& Assembler~\cite[p.~5-6]{Dandamudi2005} % TODO: include figures describing how low-level assembly is?
%     \item Stack (with figure) \& Heap (briefly)~\cite[p.40]{Patterson2017}
% 	\item Briefly explain what calling conventions are~\cite[p.42]{Patterson2017}
% \end{itemize}

\subsection{Sections of an ELF File}
Since a program needs to be representable in a low-level manner, special formats are often required.
ELF stands for \enquote{executable and linkable format} and is often found on unix-like systems, e.g. Linux.
Programs using the ELF format can be represented in three different types of files.
For instance, object files generated by a compiler, like in the previous LLVM section, might use the ELF format.
Furthermore, libraries using \emph{shared object files} might also leverage the ELF format.
Most executable program use the format in order to represent a structured container for instructions, data, and additional information.
This way, the unit is mostly self-contained an can be executed by the operating system easily.
Therefore, ELF describes the format of a class of files and not just of an individual type of file~\cite[p.~74-76]{Zhirkov2017-wk}.

Even though a processor only has access to one physical memory unit for both instructions and program data,
most assembly programs like to separate these types of memory into their separate components.
Therefore, an object file and assembly program is divided into so-called \emph{sections}~\cite[p.~19]{Zhirkov2017-wk}.

In ELF programs, important sections which are later used include:
\begin{itemize}
	\item \texttt{.text} stores the logic of the program represented using CPU instructions
	\item \texttt{.rodata} stores read-only global data, it is often used for global constants.
	\item \texttt{.data} stores mutable global data, such as mutable global variables
\end{itemize}

It is to be mentioned that this list only includes entries which are of importance later in later sections of the paper~\cite[p.~76]{Zhirkov2017-wk}.

\subsection{Assemblers and Assembly Language}

\begin{wrapfigure}{R}{.27\textwidth}
	\centering
	\begin{tikzpicture}[node distance=1cm, inner sep=3mm]
        \node (lexical_analysis) [entity, fill=none] {High-level languages\\(\emph{C}, \emph{Rust}, \emph{rush})};
		\node (syntactic_analysis) [entity, below=of lexical_analysis, align=center] {Assembly language\\level};
		\draw [arrow] (lexical_analysis) -- (syntactic_analysis);
		\node (semantic_analysis) [entity, fill=none, below=of syntactic_analysis, align=center] {Machine language\\level};
		\draw [arrow] (syntactic_analysis) -- (semantic_analysis);
		\node (codegen) [entity, fill=none, below=of semantic_analysis] {Operating system \\ and Hardware level};
		\draw [arrow] (semantic_analysis) -- (codegen);
	\end{tikzpicture}
	\caption{Level of Abstraction provided by Assembly}\label{fig:abstractions}
\end{wrapfigure}

Assembly language describes a type of low-level programming languages which are directly influenced by the target architecture.
Since the assembly code provides a slight abstraction over the computer's hardware,
the assembly code must be translated to machine code before it can be executed.
This process is performed by a program named the \emph{assembler} and is often called \emph{assembly}.
A typical characteristic of assembly code is that it seems relatively cryptic to a human reading it.
Furthermore, compared to high-level languages like C, assembly code is relatively low-level since it can be used to interact with the hardware directly.

For instance, the RISC-V instruction \qVerb{add a0, a0, 2} would be used in integer addition.
This example contains most characteristics of an assembly language program.
Like hinted previously, the name of the instruction is a mnemonic.
In this case, \qVerb{addi} stands for \enquote{add immediate}.
Furthermore, the exact semantic meaning of the instruction is not immediately apparent.
At last, the instruction for adding integers differs for most CPU architectures.
For instance, an equivalent instruction for the x\_86 architecture could be `\texttt{add \%rdi, 2}'.
Therefore, the fact that instructions differ on each target architecture is clearly apparent.

In order to understand how much abstraction is provided by assembly, we should consider Figure~\ref{fig:abstractions}.
Here, the highest level of abstraction is provided by high-level programming languages like C or Rust.
In the context of this paper, rush presents roughly the same level of abstraction like these languages.
The next lower level of abstraction is provided by assembly.
Now, the program is no longer independent of its target architecture and is much more demanding to furmulate.
However, the next lower level of abstraction below assembly is represented by code in machine language.
As of today, one only rarely encounters a programmer writing programs using this level of abstraction.
Since the machine language program is represented in binary, it is nearly impossible for a human to write or understand.
However, the machine language is also just an abstraction of the computer's hardware and operating system.
Therefore, assembly provides enough abstraction to be comprehensible for a human whilst being a low-level representation of the program.
Although assembly provides more abstraction than the two levels at the bottom of the figure, programmers rarely program in assembly directly.
Some of the benefits of using assembly language to formulate a program are increased runtime efficiency and decreased code size.
Therefore, it might be reasonable for a compiler to generate assembly code from the source program.
This way, the program is translated into a low-level, target-specific representation which allows the program to be executed on the target machine directly.
However, an assembler is still required in order to translate the assembly output of the compiler.
Since most assemblers output object files, a linker is required to create the final executable program.
Therefore, a compiler targeting the assembly of a specific architecture depends on these two additional steps before the program can be executed~\cite[p.~5-6]{Dandamudi2005}.

One might argue that the compiler could output object files directly.
However, doing so rarely creates any significant benefits other than the omitted dependence on the assembler.
Furthermore, implementing a compiler using this approach often significantly increases the complexity of the compiler since it now has to perform the role of the assembler as well.
However, the compiler could also emit binary data directly, thus making its implementation significantly more demanding.

\subsection{Registers}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\begin{tikzpicture}[node distance=2cm]
		\node(CPU)[center] {CPU};
		\node(registers)[entity, left of=CPU, yshift=3cm, vstack=4, rectangle split part align=left] {
			Registers
			\nodepart{two}{\texttt{r0}}
			\nodepart{three}{\texttt{r1}}
			\nodepart{four}{\ldots}
		};
		\node(memory)[entity, right of=CPU, yshift=3cm] {Memory};

		% TODO: Maybe use `arrow` instead of `relation` in order to remove spacing
		\draw [darrow, very thick, double] (CPU) -- node[anchor=east, yshift=-.1cm] {fast} (registers);
		\draw [darrow] (CPU) -- node[anchor=west] {slow} (memory);
	\end{tikzpicture}
	\caption{Relationship Between Registers, Memory, and the CPU}\label{fig:cpu_reg_mem}
\end{wrapfigure}

Most processors contain numerous registers in order to hold data, instructions, and state information.
In a computer, registers are sometimes used to hold data stored in variables.
However, most of the time, registers are used as temporary storage in large computations.
Registers are used in the latter case because they are way faster than memory.
Therefore, an algorithm using registers instead of memory will usually run faster.
However, even though registers are faster than memory, they are not used all the time.
This is because CPU registers are a limited resource, meaning every register-based CPU only has a finite amount of registers available.
Registers should therefore be used carefully and only when they are really needed~\cite[pp.~212-214]{Watson2017}.

Figure~\ref{fig:cpu_reg_mem} shows how the CPU interacts with its registers and the computer's memory.
The connection between the set of registers and the CPU is marked as a short and thick arrow because the connection between the CPU and its registers is very short and fast.
Although the registers are often physically parts of the CPU, they are displayed as separate entities in this example.
For the memory, the arrow is long and thin.
This represents the long and therefore slow connection between the CPU and the memory modules.
In most modern computers, this connection often spans across several centimeters on the motherboard.
Therefore, higher latency during memory access is inevitable~\cite[pp.~20-21]{Dandamudi2005}.

Nearly every CPU architecture will include an individual register layout, differing in size, count, and type.
Therefore, the exact information about registers always depends on which CPU is used.
As a rule of thumb, having more registers in an architecture will almost always improve performance of that architecture.
Furthermore, not every register is identical.
There might be platforms with some general-purpose, some floating-point, and some special-purpose registers.
For instance, there may be registers which contain information about the CPU's current instruction.
Furthermore, common architectures also include special-purpose registers for modifying the machine's memory.
Therefore, there is nearly always an appropriate register matching the requirement~\cite[Chapter~2]{Dandamudi2005}.

\Lirsting[wrap=L, raw=true, ranges={4-6}, wrap width=0.3\textwidth, caption={Example Assembly Program for Explaining Register Allocation}, label={lst:register_alloc}]{listings/register_alloc_simple.s}

For a compiler, the limited amount of registers presents a big challenge.
Some architectures may require that the operands of arithmetic or logical instructions have been explicitly loaded into registers beforehand.
In this case, registers would be required in nearly all computations.
In order to understand how registers management can present a challenge, Listing~\ref{lst:register_alloc} should be considered.
This snippet displays RISC-V assembly instructions which calculate the sum of the two integets 40 and 2.
In line 4, the integer value 40 is placed in the register \qVerb{a0}.
In line 5, another integer, this time 2 is placed in \qVerb{a1}.

Next, the \qVerb{add} instruction is used to calculate the sum of these two integers.
Since the first operand of the instruction specifies the register in which the result should be placed,
the original value of 40 in the register \qVerb{a0} would be overwritten by the instruction~\cite[reference]{Patterson2017}.
Through this example, it becomes apparent that other instructions might use registers unexpectedly.
Therefore, subtle bugs can be created if an instruction overwrites registers.

In compilers, the process of \emph{register allocation} is responsible for managing how registers are used.
This process attempts to use registers in a way leading to maximized efficiency of the output program.
Since accessing registers is often faster, a registers allocation algorithm will try to use registers as often as possible.
Production-ready compilers often try to keep as much of the frequently accessed data in registers.
For instance, this frequently accessed data may also include variables which are normally saved in memory.
It is apparent that in most programs, the number of variables will certainly exceed the capacity provided by registers.
Therefore, register allocation has to detect when no free registers are available anymore.
In this case, the compiler has to save data in memory instead of registers.
This process of saving excess data in memory instead of registers is called \emph{register spilling}.
Since register spilling introduces a performance penalty, register allocation algorithms often attempt to prevent it as much as possible.
Therefore, sophisticated algorithms for register allocation are often mandatory as long as the factor of output code performance is non-trivial.

Apart from just managing the use of registers, most allocation algorithms are responsible for many other register-related tasks.
For instance, register allocation should detect when a variable is no longer needed so that its register can be freed.
Moreover, register allocation has to ensure that no conflicts between registers are introduced.
Such a conflict may be that a register is accidentally overwritten by an instruction in a completely unrelated basic block.
It is apparent that an algorithm performing all these tasks can not be implemented in an ad-hoc manner.
Instead, this process often requires complex graph algorithms for determining which registers can be used and freed.
Therefore, implementing register allocation in a compiler is often a very demanding task~\cite[pp.212-214]{Watson2017}.
Since register allocation represents a complex topic, it will not be explained any further.

% \begin{itemize}
% 	\item Processors contain numerous registers in order to hold data, instructions, and state information X
% 	\item Registers are required for two reasons: 1. Very fast storage 2. Most instructions require that the operands are in registers X
% 	\item Precious resource have to be used carefully X
% 	\item \cite[p.~212-214]{Watson2017}
%
% 	\item Include graphic of computer with registers and memory X
% 	\item Largely dependent on the type of CPU used X
% 	\item A large number of registers often improves performance X
%
% 	\item RISC architectures typically have a large number of registers
% 	\item Most CPUs have special-purpose registers
% \end{itemize}

% Watson 2017: p. 212
% Dandamudi2005: Chapter 2


\subsection{Using Memory: The Stack and the Heap}
\TODO{@RubixDev}

\begin{itemize}
	\item Repetition: When is the stack used instead of registers?
	\item Insert figure of the stack
\end{itemize}

\subsection{Calling Conventions}

Programmers often use \emph{procedures} or \emph{functions} in order to structure their programs,
both to make the program easier to understand and to allow shared logic to be reused.
Therefore, procedures allow the programmer to focus on one portion of the tasks at the time.
Often, parameters and returned values act as the interface between the procedure and the remaining code.
Therefore, procedures present one way of how a high-level programming language provides abstraction.
During a procedure call, like a function call in rush, many individual steps need to be performed in the program's assembly representation.
Since assembly does not support the use of high-level functions, most architectures specify their own \emph{calling convention} which describes how low-level procedure calls are to be performed.
In short, a procedure call in RISC-V assembly consists of the following six steps.

\begin{itemize}
	\item Placing the arguments for the procedure's parameters in a place were the procedure can access them.
	\item Transferring control to the procedure using a jump or call instruction.
	\item Acquiring local storage resources the procedure needs, spilling register if required.
	      This step also often involves allocating stack memory for the procedure's local variables.
	\item The procedure performed its internal logic by executing its instructions.
	\item It now places its result value in a place where the caller can access it.
	      In this step the resources allocated in step 3 are also freed and released.
	\item Returning control back to the caller code using a return instruction since procedures can be called from multiple places in the program.
\end{itemize}

In order to leverage optimal performance during function calls, passed arguments are usually kept in registers.
However, in the previous subsection about registers, we have learned that most architectures state that only subset of their registers are to be used as function arguments.
Therefore, if a function is called using more than eight arguments, every remaining argument has to be spilled to memory~\cite[p.~98]{Patterson2017-zq}.

\TODO{explain prologue and epilogue}


\TODO{RISC vs. CISC~\cite[p.~5-6]{Dandamudi2005}}
% TODO: the below goes under conclusions
% \section{CISC and RISC Architectures}
%
% \subsubsection{CISC}
% % CISC was chosen to be the first one on purpose
% \begin{itemize}
% 	\item X86 (64) as an example (briefly)
% 	\item CISC architectures date back to the time in which Assembly was written by hand.
% 	      Programs were easier to write and often performed better due to these complex instructions.
% 	      Larger quantity of instructions in CISC architectures.
% 	\item Complex \& many instructions cause the architecture to be a bit slower for compiled programs
% \end{itemize}
%
% \subsubsection{RISC}
%
% \begin{itemize}
% 	\item RISC-V as an example (briefly)
% 	\item Rather modern
% 	\item Better for modern CPUs which mostly execute machine-written code
% 	\item Harder to write manually
% 	\item Easier to implement a compiler for
% 	\item Usually small and very simple instruction set
% 	\item Gain in popularity (ARM / RISC-V)
% \end{itemize}
