% chktex-file -2
\section{Lexical and Syntactical Analysis}

As previously mentioned, the first step during compilation or program execution is the \emph{lexical} and \emph{syntactical analysis}.
Program source text is, without previous processing, just \emph{text} i.e., a sequence of characters.
Before the computer can even begin to analyze the semantics and meaning of a program it has to first \emph{parse} the program source text into an appropriate data structure.
This is done in two steps that are closely related and often combined, the \emph{lexical analysis} performed by a \emph{lexer} and the \emph{syntactical analysis} performed by a \emph{parser}.

\subsection{Formal Syntactical Definition by a Grammar}

Just like every natural language, most programming languages also conform to a grammar.
However, grammars for programming languages most often are of type 2 or 3 in the Chomsky hierarchy, that is \emph{context-free} and \emph{regular} languages, whereas natural languages often are type 1 or 0~\cite[p.~24]{Watson2017}.
Additionally, it is not uncommon for parser writers to formally define the grammar using some notation.
Popular options include \emph{BNF}\footnote{Backus-Naur Form, named after the two main inventors~\cite{Backus1960}} and \emph{EBNF}\footnote{Extended Backus-Naur Form, an extended version of \emph{BNF} with added support for repetitions and options without relying on recursion, first proposed by Niklaus Wirth in 1977~\cite{Wirth1977} followed by many slight alterations. The version used in this paper is defined by the ISO/IEC 14977 standard.}, the latter of which we use here.
This paper does not fully explain these notations, however Listing~\ref{lst:ebnf_reference_grammar} shows a short example grammar notated using EBNF.
For reference, Appendix~\ref{apx:grammar} contains the full grammar of rush.
\TODO{example for type 1 language `\Verb{([)]}'?}
\TODO{explain start symbol?}
\TODO{formal definition of grammars $G=(N,T,S,P)$?}

\Lirsting[float=h, caption={Grammar for Basic Arithmetic in EBNF Notation}, label={lst:ebnf_reference_grammar}]{listings/ebnf_reference.ebnf}

One thing worth explaining is the `\Verb{-}' at the end of line 5.
It is used to exclude a set of symbols from the preceding rule.
As there is no symbol following the dash in this case, the empty word, called $\epsilon$, is excluded from the preceding repetition.
Thus, the repetition cannot be repeated zero times here, as that would produce the empty word, which is excluded.
So the notation `\Verb{{ ... }-}' is used to represent repetitions of one or more times, whereas the same without the trailing dash represents repetitions of zero or more times.

\subsection{Grouping of Characters Into Tokens}

Before the syntax of a program is validated it is common to have a lexer group certain sequences of characters into \emph{tokens}.
The set of tokens a language has is the union of the set of all terminal symbols used in context-free grammar rules and the set of regular grammar rules.
For the language defined in Listing~\ref{lst:ebnf_reference_grammar} these are the five operators `\Verb{+}', `\Verb{-}', `\Verb{*}', `\Verb{/}', `\Verb{**}', and the `integer' non-terminal.

\Lirsting[ranges={10-16}, wrap=L, wrap width=0.4\textwidth, caption={The rush \qVerb{Lexer} Struct Definition}, label={lst:lexer}]{deps/rush/crates/rush-parser/src/lexer.rs}

The specifics of implementing a lexer are not explored in this paper, however a basic overview is still provided.
The base principal of a lexer is to iterate over the characters of the input to produce tokens.
Depending on the target language it might however be required to scan the input using an $n$-sized window i.e., observing $n$ characters at a time.
In the case of rush this $n$ is 2, resulting in the \Verb{Lexer} struct not only storing the current character but also the next character as seen in Listing~\ref{lst:lexer}.
For clarity, Table~\ref{tbl:lexer} shows the values of \Verb{curr_char} and \Verb{next_char} during processing of the input `\Verb{1+2**3}'.
Here every row in the table represents one point in time displaying the lexer's current state.

\begin{table}[h]
	\newcommand{\lexerframe}[8][]{\tikz[baseline={(frame.base)}]{
			\node(frame)[stack=6, rectangle split part fill={#2}, #1]{
				\texttt{#3}
				\nodepart{two}\texttt{#4}
				\nodepart{three}\texttt{#5}
				\nodepart{four}\texttt{#6}
				\nodepart{five}\texttt{#7}
				\nodepart{six}\texttt{#8}
			};
			% fix margin around tikzpicture
			\node[fit={(current bounding box)}, inner ysep=.5mm, inner xsep=0]{};
		}}
	\centering
	\caption{Advancing Window of a Lexer}\label{tbl:lexer}
	\begin{tabularx}{0.95\textwidth}{rCLLl}
		Calls & State                                                                                                                                                     & \colorbox{black!20}{\Verb{curr_char}} & \colorbox{black!10}{\Verb{next_char}} & Output Token  \\
		\\[-1.1em]\hline\\[-1.1em]
		0     & \lexerframe{none}{1}{+}{2}{*}{*}{3}                                                                                                                       & \Verb{None}                           & \Verb{None}                           &               \\
		0     & \lexerframe{black!10,none}{\bfseries1}{+}{2}{*}{*}{3}                                                                                                     & \Verb{None}                           & \Verb{Some('1')}                      &               \\
		0     & \lexerframe{black!20,black!10,none}{\bfseries1}{\bfseries+}{2}{*}{*}{3}                                                                                   & \Verb{Some('1')}                      & \Verb{Some('+')}                      &               \\
		1     & \lexerframe{none,black!20,black!10,none}{\color{black!50}1}{\bfseries+}{\bfseries2}{*}{*}{3}                                                              & \Verb{Some('+')}                      & \Verb{Some('2')}                      & \Verb{Int(1)} \\
		2     & \lexerframe{none,none,black!20,black!10,none}{\color{black!50}1}{\color{black!50}+}{\bfseries2}{\bfseries*}{*}{3}                                         & \Verb{Some('2')}                      & \Verb{Some('*')}                      & \Verb{Plus}   \\
		3     & \lexerframe{none,none,none,black!20,black!10,none}{\color{black!50}1}{\color{black!50}+}{\color{black!50}2}{\bfseries*}{\bfseries*}{3}                    & \Verb{Some('*')}                      & \Verb{Some('*')}                      & \Verb{Int(2)} \\
		4     & \lexerframe{none,none,none,none,black!20,black!10}{\color{black!50}1}{\color{black!50}+}{\color{black!50}2}{\color{black!50}*}{\bfseries*}{\bfseries3}    & \Verb{Some('*')}                      & \Verb{Some('3')}                      &               \\
		4     & \lexerframe{none,none,none,none,none,black!20}{\color{black!50}1}{\color{black!50}+}{\color{black!50}2}{\color{black!50}*}{\color{black!50}*}{\bfseries3} & \Verb{Some('3')}                      & \Verb{None}                           & \Verb{Pow}    \\
		5     & \lexerframe{none}{\color{black!50}1}{\color{black!50}+}{\color{black!50}2}{\color{black!50}*}{\color{black!50}*}{\color{black!50}3}                       & \Verb{None}                           & \Verb{None}                           & \Verb{Int(3)} \\
	\end{tabularx}
\end{table}

As explained in Section~\ref{sec:stages_of_compilation}, many modern language implementations have the lexer produce tokens on demand.
Thus, a lexer requires one public method called something like \Verb{next_token} reading and returning, as the name suggests, the next token.
In Table~\ref{tbl:lexer} the column on the left displays how many times the \Verb{next_token} method has been called by the parser.
In the first three rows this count is still 0, as this happens during initialization of the lexer in order to fill the \Verb{curr_char} and \Verb{next_char} fields with sensible values before the first token in requested.
The \Verb{Pow} token, composed of two `\Verb{*}' symbols, requires the lexer to advance two times before it can be returned, which is represented by the two rows in which the call count is 4.
A simplified \Verb{Token} struct definition for the example language from Listing~\ref{lst:ebnf_reference_grammar} is shown in Listing~\ref{lst:token_simple}.

\Lirsting[wrap=L, wrap width=0.3\textwidth, caption={Simplified \qVerb{Token} Struct Definition}, label={lst:token_simple}]{listings/token_simple.rs}

In addition to the current and next character, a lexer also has to keep track of the current position in the source text for it to provide helpful diagnostics with locations to the user.
This is done in the \Verb{location} field which is incremented every time the lexer advances to the next character.
While producing a token the lexer can then read this field once at the start and once after having read the token and save the two values in the token's span.

A special case worth mentioning are comments.
As explained later in Section~\ref{sec:constructing_a_tree}, depending on the parser, comments may be simply ignored and skipped during lexical analysis, or get their own token kind and be treated similar to string literals.

\TODO{maybe mention having spans inclusive or exclusive}

\subsection{Constructing a Tree}\label{sec:constructing_a_tree}

The parser uses the generated tokens in order to construct a tree representing the program's syntactic structure.
Depending on how the parser should be used, this can either be a \emph{Concrete Syntax Tree} (CST) or an \emph{Abstract Syntax Tree} (AST).
The former still contains information about all input tokens with their respective locations, while the latter only stores the abstract program structure with just the relevant information for basic analysis and execution.
Therefore, a CST is usually used for development tools like formatters and intricate linters and analyzers where it is important to preserve stylistic choices made by the programmer or to know the exact location of every token.
However, an AST is enough for interpretation and compilation as it preserves the semantic meaning of the program.
Figure~\ref{fig:parser_simple_ast} shows an AST for the program `\Verb{1+2**3}' in the language notated in Listing~\ref{lst:ebnf_reference_grammar} on page~\pageref{lst:ebnf_reference_grammar}.
In the case of rush an AST with limited location information is used, because rush's semantic analyzer is still basic enough to work with that, and, as discussed, execution and compilation requires no CST.

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\begin{tikzpicture}[level distance=1cm, sibling distance=2cm]
		\node {Expression}
		child {node {Term}
				child {node {Factor}
						child {node {\Verb{Int(1)}}}}}
		child {node {\Verb{Plus}}}
		child {node {Term}
				child {node {Factor}
						child {node {\Verb{Int(2)}}}
						child {node {\Verb{Pow}}}
						child {node {Factor}
								child {node {\Verb{Int(3)}}}}}};
	\end{tikzpicture}
	\caption{Abstract Syntax Tree for `\texttt{1+2**3}'}\label{fig:parser_simple_ast}
\end{wrapfigure}

Not every parser is the same and there are a few different strategies for implementing one.
These strategies are categorized into \emph{top-down parsers} and \emph{bottom-up parsers}.
The main difference between them being the kind of tree traversal they perform.
Top-down parsers construct the syntax tree in a pre-order manner, meaning a parent node is always processed before its children nodes.
Hence, the syntax tree is constructed from top to bottom, starting with the root node.
Bottom-up parsers instead perform post-order traversal.
That way, all child nodes are processed before their parent node.
This results in the tree being constructed from the leafs at the bottom upwards to the root.

Top-down and bottom-up parsers are further categorized into many more subcategories.
The two we will focus on here are so-called \emph{LL$(k)$} parsers and \emph{LR$(k)$} parsers.
\TODO{`we will' ok?}
These are named after the direction of reading the tokens, `L' being from left to right, and the derivation they use.
`L' is the leftmost derivation and `R' the rightmost derivation.
\TODO{what are derivations? + citation can be found here \cite[pp.~75f]{Watson2017}}
The parenthesized $k$ represents a natural number with $k\in\mathbb{N}_0$ describing the number of tokens for \emph{lookahead}.
Often $k$ is either $1$ or $0$ for a two or one wide window respectively.
This window moves just like previously explained for the lexer, and observes $k$ \textbf{tokens}, not characters, simultaneously.
Since in most cases $k$ is $1$, it is common to omit specifying it and to just speak of `LL' and `LR' parsers~\cite[pp.86-88]{Watson2017}.

Alternatively to utilizing lookahead tokens, it is possible to create parsers with backtracking.
Using that approach, a parser has to guess which syntax construct follows, if it can't concretely know based on the first token.
When it then later detects an unexpected symbol, instead of throwing a syntax error, it cancels and returns to the point of decision-making to try the next possible construct.
This of course comes with overhead and usually unclear error messages, which is why this method is rarely used and the superior lookahead method is preferable.

An example for LR parsing is the \emph{shift-reduce} parsing approach, which is partially explained later on page~\pageref{sec:parser_generators}.
One thing to note however, is that LR parsers are generally very complicated to implement manually.
LL parsers on the contrary are usually much simpler to implement, but come with a limitation.
By design, they must recognize a node by its first $n=k+1$ tokens, where $n$ is the window size.
However, due to that restriction, not every context-free language can be parsed by an LL parser.
An example for that is given in Listing~\ref{lst:ebnf_reference_pratt}.

\Lirsting[float=h, caption={Example Language a Traditional LL$(1)$ Parser Cannot Parse}, label={lst:ebnf_reference_pratt}]{listings/ebnf_reference_pratt.ebnf}

Most LL parsers are \emph{recursive-descent} parsers, including the rush parser.
Implementation of such a recursive-descent parser is rather uncomplicated.
Assuming the grammar respects the mentioned limitation, every context-free grammar rule is mapped to one method on a \Verb{Parser} struct.
In the example grammar from Listing~\ref{lst:ebnf_reference_grammar} on page~\pageref{lst:ebnf_reference_grammar} again, these are all the capitalized rules highlighted in yellow.
Additionally, a matching node type is defined for each context-free rule, holding the relevant information for execution.
In Rust the mapping from EBNF grammar notation to type definitions is very simple as displayed in Table~\ref{tbl:ebnf_to_rust}.
\TODO{struct signature example?}

\begin{table}[h]
	\caption{Mapping From EBNF Grammar to Rust Type Definitions}\label{tbl:ebnf_to_rust}
	\rowcolors{2}{gray!15}{}
	\begin{tabularx}{0.95\textwidth}{Ll}
		\rowcolor{gray!25} EBNF                         & Rust                                                                         \\
		\hline
		\LirstInline{ebnf}{A = B , C ;}                 & \LirstInline{rs}{struct A { b: B, c: C }}                                    \\
		\LirstInline{ebnf}{A = B , [ C ] ;}             & \LirstInline{rs}{struct A { b: B, c: Option<C> }}                            \\
		\LirstInline{ebnf}{A = B , { C } ;}             & \LirstInline{rs}{struct A { b: B, c: Vec<C> }}                               \\
		\LirstInline{ebnf}{A = B , { C }- ;}            & \LirstInline{rs}{struct A { b: B, c: Vec<C> }}                               \\
		\LirstInline{ebnf}{A = B | C ;}                 & \LirstInline{rs}{enum A { B(B), C(C) }}                                      \\
		\LirstInline{ebnf}{A = B , ( '+' | '-' ) , C ;} & \gape{\makecell[l]{\LirstInline{rs}{struct A { b: B, op: Op, c: C }}         \\\LirstInline{rs}{enum Op { Plus, Minus }}}} \\
		\LirstInline{ebnf}{A = B , [ ( X | Y ) , C ] ;} & \gape{\makecell[l]{\LirstInline{rs}{struct A { b: B, c: Option<(XOrY, C)> }} \\\LirstInline{rs}{enum XOrY { X(X), Y(Y) }}}} \\
	\end{tabularx}
\end{table}

\subsubsection{Operator Precedence}

As previously discussed, a traditional LL parser cannot parse the language from Listing~\ref{lst:ebnf_reference_pratt}.
However, when comparing it to Listing~\ref{lst:ebnf_reference_grammar} it might become obvious that the two grammars notate the same language.
The first one simply provides additional information about the order of nesting different kinds of expressions, called \emph{precedence}.
For example, when parsing the expression `\Verb{1+2*3}' the `\Verb{2*3}' part should be nested deeper in the tree for it to be evaluated first.
In Listing~\ref{lst:ebnf_reference_grammar} this is achieved by recognizing multiplicative expressions as \Verb{Term}s and having additive expressions be composed of \Verb{Term}s.
Listing~\ref{lst:ebnf_reference_pratt} does not indicate the order of evaluation itself, so it must be provided externally.

Additionally, a precedence may be either left or right associative.
Consider the input `\Verb{1*2*3}' it should be evaluated from left to right, so first `\Verb{1*2}' and then the result times 3.
Now consider `\Verb{1**2**3}'\footnote{`\Verb{**}' is the power operator here, so the input would be written as $1^{2^3}$ using mathematical notation}.
Here the `\Verb{2**3}' should be evaluated first and afterwards 1 should be raised to the result.
That means while most operators are evaluated from left to right, that is, they are left associative, some operators like the power operator are evaluated from right to left and are therefore right associative.
In Listing~\ref{lst:ebnf_reference_grammar} left associativity is achieved by allowing simple repetition of the operator for an indefinite amount of times.
Right associativity instead uses recursion on the right-hand side of the operator.

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\begin{tikzpicture}[level distance=1cm, sibling distance=2cm]
		\node {Expression}
		child {node {Expression}
				child {node {\Verb{Int(1)}}}}
		child {node {\Verb{Plus}}}
		child {node {Expression}
				child {node {Expression}
						child {node {\Verb{Int(2)}}}}
				child {node {\Verb{Pow}}}
				child {node {Expression}
						child {node {\Verb{Int(3)}}}}};
	\end{tikzpicture}
	\caption{Abstract Syntax Tree for `\texttt{1+2**3}' Using Pratt-Parsing}\label{fig:parser_simple_ast_pratt}
\end{wrapfigure}

For LR parsers the precedence and associativity for each operator is encoded within the parser table.
However, there is also a method called \emph{Pratt-Parsing} that allows slightly modified recursive-descent LL parsers to parse such languages, given a map from tokens to precedences and their associativity.
Often the grammars without included precedence are preferred, because they usually result in a simpler structure of the resulting syntax tree.
This can be seen when comparing Figure~\ref{fig:parser_simple_ast} from earlier to Figure~\ref{fig:parser_simple_ast_pratt} which shows the resulting AST for the same input using the alternative language representation.
Most notably, the rather long sequences of nodes with just a single child, like the path on the left simply resolving to a single \Verb{Int(1)} token, are gone in Figure~\ref{fig:parser_simple_ast_pratt}.

\subsubsection{Pratt-Parsing}

As the rush parser makes use of Pratt-Parsing, most of the following code snippets are taken from there.
First a mapping from a token kind to its precedence must be defined.
The one for rush is found in Listing~\ref{lst:rush_tok_prec}.
It shows the \Verb{prec} method implemented on the \Verb{TokenKind} enum.
The return type is a tuple of two integers, one for left and one for right precedence.
For all but one token kind the left precedence is lower than the right one, resulting in left associativity.
The higher the precedences are, the deeper in the tree the resulting expressions will be, and the earlier they are evaluated.
All unrelated tokens are simply assigned a precedence of 0 for left and right.

\Lirsting[ranges={172-173, 194-201}, float=h, label={lst:rush_tok_prec}, caption={Token Precedences in rush}]{deps/rush/crates/rush-parser/src/token.rs}

The \Verb{expression} method on the \Verb{Parser} struct is then modified to take a parameter for the current precedence as seen in Listing~\ref{lst:pratt_expr}.
It then first matches on the current token kind to decide which expression to parse and stores the result in the \Verb{lhs}\footnote{Short for `left-hand side'} variable.
Afterwards it checks whether the left precedence of the now current token is bigger than the \Verb{prec} argument.
When called from elsewhere, like in the condition of a while-loop or in a grouped expression, the \Verb{prec} argument has its minimum value of 0 as shown in Listing~\ref{lst:pratt_grouped_expr}.
In that case, this check will only fail when the whole expression is over, as every non-operator token is assigned a precedence higher than 0.
If it does not fail, the \Verb{infix_expr} method is called with the matching operator and the \Verb{lhs}.
Afterwards \Verb{lhs} is overwritten with the returned value.

\Lirsting[ranges={551-555, 568-570, _574-583, 613-618}, float=h, label={lst:pratt_expr}, caption={Pratt-Parser: Implementation for Expressions}]{deps/rush/crates/rush-parser/src/parser.rs}

\Lirsting[ranges={733-743, 749-749}, float=h, label={lst:pratt_grouped_expr}, caption={Pratt-Parser: Implementation for Grouped Expressions}]{deps/rush/crates/rush-parser/src/parser.rs}

The \Verb{infix_expr} method in Listing~\ref{lst:pratt_infix_expr} simply stores the right \TODO{unclear it's the side `right', not `correct'} precedence of the operator token, advances to the next token, and calls the \Verb{expression} method again for its right-hand side, but this time with the stored \Verb{right_prec} as the minimum precedence.
These simple calls and checks of precedences automatically result in correct grouping and nesting of the expressions.

\Lirsting[ranges={751-751, _756-759, 770-770}, float=h, label={lst:pratt_infix_expr}, caption={Pratt-Parser: Implementation for Infix Expressions}]{deps/rush/crates/rush-parser/src/parser.rs}

\begin{figure}[hb]
	\newcommand{\precs}[5][black]{
		\node(tok_#3)[below of=#3]{#2};
		\draw[arrow](#3) -- (tok_#3);

		\node(#3_lprec)[prec, color=#1, below of=tok_#3, xshift=-4mm]{#4};
		\node(#3_rprec)[prec, color=#1, below of=tok_#3, xshift= 4mm]{#5};
		\draw[arrow, color=#1](tok_#3) -- (#3_lprec);
		\draw[arrow, color=#1](tok_#3) -- (#3_rprec);
	}
	\centering
	\begin{tikzpicture}[prec/.style={font=\footnotesize}]
		\node(lparen)                             {\Verb{(}};
		\node(1)     [xshift=5mm, right of=lparen]{\Verb{1}};
		\node(+)     [xshift=5mm, right of=1]     {\Verb{+}};
		\node(2)     [xshift=5mm, right of=+]     {\Verb{2}};
		\node(*)     [xshift=5mm, right of=2]     {\Verb{*}};
		\node(3)     [xshift=5mm, right of=*]     {\Verb{3}};
		\node(rparen)[xshift=5mm, right of=3]     {\Verb{)}};
		\node(/)     [xshift=5mm, right of=rparen]{\Verb{/}};
		\node(4)     [xshift=5mm, right of=/]     {\Verb{4}};
		\node(**)    [xshift=5mm, right of=4]     {\Verb{**}};
		\node(5)     [xshift=5mm, right of=**]    {\Verb{5}};

		\precs[gray]{\Verb{LParen}}{lparen}{28}{29}
		\precs[gray]{\Verb{Int(1)}}{1}     {0} {0}
		\precs      {\Verb{Plus}}  {+}     {19}{20}
		\precs[gray]{\Verb{Int(2)}}{2}     {0} {0}
		\precs      {\Verb{Star}}  {*}     {21}{22}
		\precs[gray]{\Verb{Int(3)}}{3}     {0} {0}
		\precs      {\Verb{RParen}}{rparen}{0} {0}
		\precs      {\Verb{Slash}} {/}     {21}{22}
		\precs[gray]{\Verb{Int(4)}}{4}     {0} {0}
		\precs      {\Verb{Pow}}   {**}    {26}{25}
		\precs[gray]{\Verb{Int(5)}}{5}     {0} {0}
	\end{tikzpicture}
	\caption{Token Precedences for Input `\Verb{(1+2*3)/4**5}`}\label{fig:token_precs}
\end{figure}

This might be made clearer by Figure~\ref{fig:token_precs}.
It shows the tokens with their respective left and right precedence for the input `\Verb{(1+2*3)/4**5}', which represents the mathematical expression $\frac{1+2\cdot3}{4^5}$.
The for this example irrelevant precedences have been grayed out.
Starting from the left, the parser first encounters an `\Verb{LParen}' token and therefore decides to parse a `\Verb{grouped_expr}'.
Inside that method, `\Verb{expression}' is again called, again with `\Verb{prec}' being 0.
The difference is that now the current token is `\Verb{Int(1)}'.
That token is then parsed as a simple integer expression and stored in `\Verb{lhs}'.
Now, the left precedence of the `\Verb{Plus}' token, 19, is higher than `\Verb{prec}'s value of 0, so the while-loop is entered.
In there, the `\Verb{infix_expr}' method is called, which queries the right precedence of the `\Verb{Plus}' token and calls `\Verb{expression}' again, this time with a precedence of 20.
Skipping to the precedence check, again the current token's left precedence is higher than `\Verb{prec}', the precedences being 21 for the `\Verb{Star}' token and 20 for `\Verb{prec}'.
Therefore, `\Verb{infix_expression}' is called once again, which call `\Verb{expression}' again, but now during the next precedence check, the left precedence of the following `\Verb{RParen}' token, 0, is not higher than the current precedence of 22.
That means, the innermost `\Verb{expression}' call returns with a single 3.
This 3 is then used as the right-hand side for the multiplicative infix expression.
The same `\Verb{RParen}' precedence check again fails for the `\Verb{expression}' call with a precedence of 20.
Thus, the `\Verb{2*3}' part together forms the right-hand side of the addition expression.
Once more, `\Verb{RParen}'s left precedence of 0 is not larger than the initial precedence of 0, hence the parser returns to the `\Verb{grouped_expr}' call with the entire contents in the parentheses.
Here the right parenthesis is skipped, and the method returns to the outermost `\Verb{expression}' call, assigning the entire grouped expression to `\Verb{lhs}'.

In order to understand the right associativity of the `\Verb{Pow}' token and to improve intuition, the reader is advised to continue going through this procedure for the rest of the example input.
A curious reader might additionally want to parse the inputs `\Verb{1*2*3}', `\Verb{1**2**3}', and `\Verb{1+(2+3)}' by hand.
It should then become clear how Pratt-Parsing achieves parsing with precedences without it being encoded in the Grammar, and with that, the node types.

\subsubsection{Parser Generators}\label{sec:parser_generators}

For most purposes, it is generally not necessary to implement parsers, and with that, lexers, manually.
Instead, there are so-called \emph{parser generators} that generate parsers based on some specification of the desired syntax and the required precedences.
Often, parser generators define a domain specific grammar notation for the syntax specification, although some parser generators also accept traditional grammar notations as input.
% As previously discussed, implementing table-driven parsers, like the shift-reduce LR parsers, is way too error-prone to be done by hand.
% But because of the flexibility and greater language support they bring over LL parsers, it is still preferable to use such a parser for production ready languages.
Most parser generators target some variation of \emph{table-driven} LR parsers.
Table-driven parsers use a table, mapping all possible combinations of tokens on the parse stack and lookahead tokens to an action.
For shift-reduce parsers the possible actions are shifting, that is reading the next input token and pushing it to the stack, and reducing, which pops some number of elements from the stack and in place pushes a node to it.
The input syntax must therefore either be unambiguous or augmented by precedence rules, so that a complete parser table can be generated.
\TODO{mention `nom'}
