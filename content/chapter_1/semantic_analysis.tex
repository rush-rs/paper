\section{Semantic Analysis}

Often, a programming language is not just defined by its grammar
since the grammar cannot specify how programs should behave.
Therefore, a programming language's behavior is often defined by a so-called \emph{semantic specification}.
Unlike the grammar, the semantic specification does not specify the language's syntax.
Instead, this specification often describes how a program should behave during runtime.
Common rules include \emph{data types} and \emph{integer overflow behavior}.
Another example of a semantic rule is that a variable has to be defined before it is used.
Defining the semantic rules of a programming language is often a demanding task
since not all requirements are clear from the beginning.
Because the semantic rules of a programming language can not be defined formally,
a language designer often writes their specification in a natural language, meaning Chomsky type 0.
However, due to the specification being written in a natural language, the specification can sometimes be ambiguous.
Therefore, a well-written semantic specification should avoid ambiguity as much as possible.
Furthermore, this specification is often written in English
due to it being well-adopted language across several academic fields like computer science.
Since those rules define when a program is valid, they have to be checked and enforced before program compilation can start
\cite[p.~21]{a_practical_guide_compiler_construction_watson_2017}.

Because rush shares its semantic rules across all backends,
it would be cumbersome to implement semantic validation in each
backend individually. Therefore, it is rational to implement a separate
compilation step which is responsible for validating the source program's
semantics. Among other checks, the so-called \emph{semantic analyzer}\footnote{Later referred to as \enquote{analyzer}} validates types
and variable references whilst performing type annotations. The last aspect is
of particular importance since all compiler backends rely on type information at
compile time. However, in order to obtain type information, the abstract syntax
tree of the source program must be traversed, performing numerous other checks
during the process. The semantic analyzer behaves exactly like this. In order to
preserve a clear boundary between the individual compilation steps, the parser
only validates the program's syntax without performing further validation.
Therefore, the analyzer traverses the abstract syntax tree previously generated
by the parser.

In order to highlight why type information is often required at compile time, we
will consider following code example. The listing \ref{lst:analyzer-test} displays a basic rush program
calculating the sum of two integer values.
The program then uses this sum as its exit-code.
In this example, the desired exit-code of the program is five.

\TSListing[caption={A rush Program Which Adds Two Integers}, label={lst:analyzer-test}, float=H]{semantic_analysis_simple.rush}

In this example, the semantic analyzer will first check if the program contains
a \texttt{main} function. If this is not the case, the analyzer rejects the program
because it violates rush's semantic specification. Just like C or Rust, rush
requires that every program must contain exactly one \texttt{main} function. If a
programming language requires a \texttt{main} function, most top-level statements can be
completely forbidden, allowing more elegant code imports between different
source files. Furthermore, a \texttt{main} function clearly indicates the program's
start of execution, thus decreasing ambiguity. Therefore, the analyzer checks that
the \texttt{main} function takes no parameters and returns no value. However, in this
example, there is a valid \texttt{main} function. Now, the analyzer traverses the
function body of the \texttt{main} function. Since \texttt{let} statements are used to declare
new variables, the analyzer will add the variables \texttt{two} and \texttt{three} to its
current scope. However, unlike an interpreter, the analyzer does not insert the
variable's value into its scope. Instead of the concrete value, the analyzer
only considers the types of expressions. Therefore, in this example, the
analyzer remembers that the variables \texttt{two} and \texttt{three} store integer values.
This information will become much more useful when we consider line 4. Here, the
analyzer checks that the identifiers \texttt{two} and \texttt{three} refer to valid variables.
Just like most other programming languages, rush does not allow the addition
of two boolean values for example. Therefore, the analyzer checks that the operands
of the \texttt{+} operator have the same type and that this type is valid in additions.
Because this validation requires information about types, the analyzer accesses
its scope when looking up the identifiers \texttt{two} and \texttt{three}. Since those names
were previously mapped to the \texttt{int} type, the analyzer is now aware of the
operand types and can check their validity. In this case, calculating the
sum of two integers is legal and results in another integer value. Since rush's
semantic specification states that the \texttt{exit} function requires exactly one
integer parameter, the analyzer has to check that it is called correctly.
Furthermore, the analyzer validates all function calls and declarations, not
just the ones of builtin functions. Since the result of the addition is also an
integer, the analyzer accepts this program since both its syntax and semantics
are valid.

As indicated previously, most compilers require type information whilst
generating target code. For simplicity, we will consider a fictional compiler
which can compile both integer and float additions. However, the fictional
target machine requires different instructions for addition depending on the
type of the operands. For instance, integer addition uses the \texttt{intadd} instruction
while float addition uses the \texttt{floatadd} instruction. Here, type ambiguity would
cause difficulties. If there was no semantic analysis step, the compiler would
have to implement its own way of determining the types of the operands at
compile time. However, determining these types requires a complete
tree-traversal of the operand expressions. Due to the recursive design of the
abstract syntax tree, implementing this tree-traversal would require a
significant amount of source code in the compiler. However, the implementation
of this algorithm would be nearly identical accross all of rush's compiler
backends. Therefore, implementing type determination in each backend
individually would enlarge the compiler source code, thus making it harder to
understand. Since code duplication is considered inelegant, outsourcing this
algorithm into a separate component is likely the best option. As a result of
this, the semantic analyzer implements such a tree-traversal algorithm for
determining the types of subtrees. Because of the previously mentioned reasons,
rush's semantic analyzer also annotates the abstract syntax tree with type
information so that it is usable for later steps of compilation.

Before examining how the analyzer's implementation behaves in the example from above,
we should first consider the attributes which play a vital role in the analyzer.

\TSListing[first line=12, last line=26, caption={Attributes of the \texttt{analyzer} struct}, label={lst:analyzer_attr}, float=H]{rush/crates/rush-analyzer/src/analyzer.rs}

Listing \ref{lst:analyzer_attr} displays the struct fields of the semantic analyzer.
The field \texttt{functions} in line 13 maps a function name to the function's signature.
Therefore, if a function is called later in the program, the analyzer checks if the function exists and can compare if the arguments match the declared parameters.
The \texttt{scopes} field in line 16 is responsible for managing variables.
In rush, blocks using braces (\texttt{\{\}}) create new scopes.
If the analyzer enters such a block, a new scope is pushed onto the \texttt{scopes} stack.
Each scope maps a variable identifier to some variable-specific data.
For instance, the analyzer keeps track of variable types, whether variables are used later, if they are mutated, and where it was declared.
By saving this much information about each declared variable, the analyzer can produce very helpful and accurate error messages and warnings.
Such an error message is displayed in listing \ref{lst:analyer_imutable_err}.

\AnsiListing[caption={Output When Compiling An Invalid rush Program}, float=h, label={lst:analyer_imutable_err}]{non_mut_variable_error.txt}

Furthermore, the \texttt{loop\_count} field is used to validate the uses of the \texttt{break} and \texttt{continue} keywords.
Because these statements are only valid inside loop bodies, the value of \texttt{loop\_count} must be $> 0$ when such a statement is used.
This counter is incremented as soon as the analyzer begins traversal of a loop body.
After the analyzer has traversed the loop's body, the counter is decremented again.
Due to this design, nested loops do not cause issues while the validity of the above statements can be guaranteed.

In order to obtain a better understanding of how the analyzer works, we will now
consider parts of its implementation and how they behave when analyzing the
example from above.

First, all functions are traversed and analyzed.
Because a \texttt{main} function is mandatory in every rush program,
the analyzer simply checks that the \texttt{functions} map contains the \texttt{main} function after all functions have been analyzed.
Furthermore, the code in listing \ref{lst:analyzer_sig_main} validates the \texttt{main} function's signature.
\TSListing[first line=404, last line=419, caption={Analyzer Validating The Signature Of The \texttt{main} Function}, label={lst:analyzer_sig_main}, float=H]{rush/crates/rush-analyzer/src/analyzer.rs}
During traversal of function bodies, the analyzer encounters two \texttt{let} statements in line two and three.
For analyzing these types of statements, the corresponding function is called.
First, the initializing expression of the let-statement is analyzed in order to obtain information about it's result data type.
After the subtree of the expression has been traversed and analyzed, its data type is now known.
The analyzer now inserts a new entry for the variable's name (e.g. \texttt{two}) into its current scope.
Among other data, this entry includes the variable's data type, which was obtained by prior analysis of the expression.
However, we have not yet explained how type annotation works in the analyzer.
In order to get an understanding of how the analyzer determines types of variables, we must consider how expressions are traversed.
The code in listing \ref{lst:analyzer_expr} is part of the method responsible for analyzing expressions.

\TSListing[first line=1007, last line=1013, caption={Analysis Of Expressions During Semantic Analysis}, label={lst:analyzer_expr}, float=H]{rush/crates/rush-analyzer/src/analyzer.rs}

The \texttt{node} parameter specifies the expression node generated by the parser,
it does not yet contain any type information since it is yet to be analyzed.
It is also apparent that the method returns a value of the type \texttt{AnalyzedExpression},
which represents an analyzed and annotated expression.
Therefore, this method consumes a non-analyzed expression and transforms it into an analyzed version of itself.
In this function, the recursive tree-traversal algorithm used in the semantic analyzer is clearly visible.
For instance, if the current expression is a block-expression (line 1009), the responsible method is called.
Since rush allows blocks which contain expressions, a block containing another block is a legal construct.
Therefore, it is possible that the \texttt{expression} method calls itself multiple times recursively.
For most other tree-traversing methods, this recursive behavior is mostly identical as most AST nodes may contain themselves.

For simple types of expressions like integers or floats further analysis is omitted.
Since these types of expressions are constant, the function direcly returns an analyzed version of the expression.
Since we have now explained how tree traversal and analysis works in general, the question of how types are saved in the new tree remains.
\TSListing[first line=129, last line=146, caption={Obtaining The Type Of Expressions}, label={lst:expr_type_impl}, float=H]{rush/crates/rush-analyzer/src/ast.rs}
The code in listing \ref{lst:expr_type_impl} shows how the type of any kind of analyzed expression can be obtained.
For constant expressions like \verb|AnalyzedExpression::(_)|, the determination of its type is straight-forward.
Here, the \texttt{result\_type} function returns \verb|Type::Int(0)|.
In this implementation, \texttt{Type} enum saves a count which specifies the number of pointer indirections.
For instance, the rush type \texttt{**int} is represented as \verb|Type::Int(2)| because there are two levels of pointer indirection.
However, if the method is called on a constant integer expression, the resulting level of pointer indirection is zero.
Therefore, this method is able to return the types of simple constant expressions with no additional effort.
For more complex constructs like if-expressions,
the corresponding analyzed AST node saves its result type directly.
For instance, during analysis of block expressions,
the responsible function checks if the block contains a trailing expression.
If this is the case, the result type of the block expression is identical to the one of its trailing expression.
This way, the analyzer is able to get type information about each node of the tree, assuming that it has been analyzed previously.
In the case of a semantically malformed program,
the analyzer must somehow continue the tree traversal because otherwise only one error could be reported at a time.
To mitigate this issue, we have implemented the \texttt{Unknown} type.
If the analyzer encounters a type conflict where one of the conflicting types is \emph{unknown},
it does not report another error since the unknown type was caused solely by a previous error.
Therefore, errors do not cascade, meaning that an undeclared variable will not cause another type error.

After the let-statementes, the \texttt{exit} function is called.
Here, the analyzer calls the \texttt{call\_expr} function which is responsible for analyzing the validity of function calls.
First, the call parameter expressions are analyzed.
Therefore, the expression \texttt{two + three} is traversed before further analysis can proceed.
Since the identifiers on the left- and right hand side have been declared previously by the two let-statements,
obtaining their data types merely involves a lookup of the identifier names inside the current scope's hashmap.
If an unknown variable is provided, the lookup in the hashmap would yield no value, thus causing an error message to be generated at this point.
Because the values of the two variables should be added, the method \texttt{infix\_expr} is called.
This function validates several constraints.
For instance, the operands must both be of the same type.
Here, both operands are integers, thus complying with the specification.
Therefore, the analyzer accepts this infix-expression and is now aware that it yields another integer.
Now that the analysis of the call argument expression has completed, its compatibility with the declared parameter must be validated.

\TSListing[first line=1822, last line=1827, caption={Validation Of Argument Type Compatibility In The Analyzer}, label={lst:analyzer_call_exit}, float=H]{rush/crates/rush-analyzer/src/analyzer.rs}

The listing \ref{lst:analyzer_call_exit} shows parts of the \texttt{arg} function which is responsible for validating that a function call argument is compatible with the declared parameter.
In the above example, this means that the \texttt{exit} function is to be called with exactly one integer argument.
This code will produce an error message if the type of the call argument deviates from the one of the declared parameter.
However, the example from above presents a syntactically and semantically correct rush program.

Another task of the semantic analyzer can be to implement early optimization.
In compiler design, most of the optimizations are often performed with the target machine in mind.
Therefore, the effects of these target-machine dependent optimizations can excel the ones caused by earlier optimizations.
However, it is still sensible to peform trivial optimizations, such as constant folding and loop conversion inside the semantic analyzer.
For instance, the rush expression $2 + 3$ evaluates to $5$ during compile time instead of run time.
This evaluation of expressions during compile time is referred to as \emph{constant folding} \cite[p.~54]{wirth_compiler_construction_2005}.
Constant folding is often used in order to avoid the emission of otherwise redundant arithmetic instructions.
As a result of this, the compiled program will run faster since less computation is being performed when the program is executed.
In order to make such optimization possible, each expression node in the analyzed AST has a method named \texttt{constant}.

\TSListing[first line=148, last line=153, caption={Method For Determining If An Expression Is Constant}, label={lst:expr_constant_impl}, float=H]{rush/crates/rush-analyzer/src/ast.rs}

This method is responsible for determining whether an expression is constant.
The method returns \texttt{true} if its expression is a constant integer, float, boolean or char.
Other types of expression, such as a call-expression cannot be constant since such a function call may cause side effects which cannot be determined during compile time.
This method is vital for constant folding since both the left- and right hand side of infix-expressions need to be constant in order to be evaluated at compile time.

Among other optimizations implemented in the analyzer, loop transformation can also have a positive effect on the program's performance during runtime.
The listing \ref{lst:rush_true_while} displays a part of a rush program which uses a \texttt{while} loop even though a \texttt{loop} would be faster.
Listing \ref{lst:rush_faster_loop} displays the same algorithm implemented using the faster \texttt{loop}.

\TSListing[caption={Redundant \texttt{while} Loop Inside a rush Program}, label={lst:rush_true_while}, float=H]{constant_true_while.rush}
\TSListing[caption={Faster Loop Algorithm Implemented In rush}, label={lst:rush_faster_loop}, float=H]{faster_loop.rush}

The \texttt{loop} implementation is more efficient since the condition check is omitted before each iteration.
Because the \texttt{while} loop checks that its condition is true before it starts the next iteration,
the \texttt{while} loop will run slower than the \texttt{loop} in this example.
However, this is only the case because the condition of the loop is a constant \texttt{true}.
Therefore, using a condition which is always true is redundant and should therefore be omitted.
If the analyzer detects such a scenario after a while-loop was analyzed, the output node will be converted into a conventional loop.
Detection of this scenario is implemented in line 855 of listing \ref{analyzer_loops}.

\TSListing[first line=851, last line=865, caption={Loop Transformation In The Analyzer}, label={lst:analyzer_loops}, float=H]{rush/crates/rush-analyzer/src/analyzer.rs}

Another scenario in which a \texttt{while} loop can be restructured occurs if the condition always evaluates to \texttt{false}.
\TSListing[caption={\texttt{while} Loop Inside a rush Program Which Never Iterates}, label={lst:rush_true_while}, float=H]{constant_false_while.rush}
Since the loop in listing \ref{lst:rush_true_while} never iterates, it is completely redundant and can therefore be omitted entirely.
This scenario is detected in line 853 of listing \ref{lst:analyzer_loops}.
This optimization improves runtime efficiency by a small amount since the code performing the condition check will not be compiled into the output program.
Furthermore, the resulting output code will also be of slightly smaller size since the entire loop compilation can be omitted.
Therefore, implementing such trivial optimizations can significantly contribute to a more efficient output program.
However, compiler writers often implement significantly more early optimization than presented in the two examples from above.
