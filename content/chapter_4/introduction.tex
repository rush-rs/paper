In the previous chapter, compilation to high-level targets has been presented.
The previously shown compilers generated outputs which were still platform independent and portable.
However, compilers can also target a specific physical computer architecture directly, thus removing another layer of abstraction.
Implementing compilers targeting the architectures presented in this chapter has proven to be a lot more demanding.
Reasons for that mostly include target-specific constraints which were
still irrelevant in the previous chapter.
The term \enquote{low-level} is used to describe such target-specific outputs here.

\section{Low-Level Programming Concepts}
Programming using high-level languages does not require knowledge about the target architecture of the program.
In this chapter, two compilers targeting low-level assembly are presented.
In order to make the sections in which these compilers are explained more approachable,
some of the most important low-level programming concepts are explained in this section.

\subsection{Sections of an ELF File}
Since a program needs to be representable in bytes, special formats for organization are required.
\emph{ELF} stands for \enquote{executable and linkable format} and is often found on unix-like systems including Linux.
Programs using the ELF format can be represented by three different types of files.
Firstly, object files generated by a compiler, like in the previous LLVM section, might use the ELF format.
Secondly, dynamic libraries using \emph{shared object files} might also leverage the ELF format.
Thirdly, executable programs use the format in order to represent a structured container for instructions, data, and additional information.
This way, the unit is mostly self-contained and can be executed by the operating system easily.
Therefore, ELF describes the format of a class of files and not just of an individual type of file~\cite[pp.~74--76]{Zhirkov2017-wk}.

Even though a processor only has access to one physical memory unit for both instructions and program data,
most assembly programs like to separate these types of memory into their separate components.
Therefore, an object file and assembly program is divided into so-called \emph{sections}~\cite[p.~19]{Zhirkov2017-wk}.
Some of the important ELF sections are listed below~\cite[p.~76]{Zhirkov2017-wk}:

\begin{itemize}
	\item \qVerb{.text} stores the logic of the program represented using CPU instructions.
	\item \qVerb{.rodata} stores read-only global data, it is often used for global constants.
	\item \qVerb{.data} stores mutable global data, such as mutable global variables.
\end{itemize}

\subsection{Assemblers and Assembly Language}

\begin{wrapfigure}{R}{.27\textwidth}
	\centering
	\begin{tikzpicture}[node distance=1cm, inner sep=3mm]
		\node (highlevel) [entity, fill=none] {High-level languages\\(\emph{C}, \emph{Rust}, \emph{rush})};
		\node (asm) [entity, below=of highlevel, align=center] {Assembly language\\level};
		\draw [arrow] (highlevel) -- (asm);
		\node (machine) [entity, fill=none, below=of asm, align=center] {Machine language\\level};
		\draw [arrow] (asm) -- (machine);
		\node (codegen) [entity, fill=none, below=of machine] {Operating system \\ and Hardware level};
		\draw [arrow] (machine) -- (codegen);
	\end{tikzpicture}
	\caption[Level of abstraction provided by assembly.]{Level of abstraction provided by assembly~\cite[p.~5-6]{Dandamudi2005}.}\label{fig:abstractions}
\end{wrapfigure}

Assembly language describes a type of low-level programming language which are directly influenced by the target architecture.
Since the assembly code provides a slight abstraction over the computer's hardware,
the assembly code must be translated to machine code before it can be executed.
This process is performed by a program named the \emph{assembler} and is called \emph{assembly process}.

For instance, the \riscv{} assembly instruction \qVerb{add a0, a0, 2} would be used for integer addition.
Often, the name of the instructions is a mnemonic roughly describing its functionality.
In the case of \riscv{}, \qVerb{addi} stands for \enquote{\texttt{a}dd \texttt{i}mmediate}.
Lastly, the instruction for adding integers differs for most CPU architectures.
For instance, an equivalent instruction for the x86\_64 architecture is `\texttt{add \%rdi, 2}'.

As described earlier, the application code in form of instructions is placed in the \qVerb{.text} section of an ELF binary.
In most assembly dialects, the programmer is able to partition the code manually using sections.
If the assembly code is assembled to an ELF object executable, the \qVerb{.text} section should contain all the instructions.
Just like parts of the LLVM IR, the \qVerb{.text} section of an assembly program can be partitioned into blocks.
This section contains many \emph{labels} which mark the beginning of a new block of instructions.
In assembly, labels rather serve as targets for jump instructions.
However, these blocks do not come with all the constraints which are to be followed when using LLVM IR\@.
For instance, a block in assembly might even contain no instructions, does not have to be terminated and could even be terminated twice.
Here, terminating instructions refer to jump- and return-instructions.
Therefore, the constraints in assembly are much weaker than the ones introduced by LLVM\@.
Because an assembly usually omits complex optimizations, strict rules constraining the assembly code can be omitted too.

In order to understand how much abstraction is provided by assembly, Figure~\ref{fig:abstractions} should be considered.
Here, the highest level of abstraction is provided by high-level programming languages like Rust, C and rush.
Compared to the aforementioned languages, rush presents roughly the same level of abstraction.
The next lower level of abstraction is represented by assembly languages.
As of today, one only rarely encounters a programmer writing programs using this level of abstraction or less.
Here, the program is no longer independent of a target architecture and consists of significantly more code.
However, the next lower level of abstraction below assembly is machine language.
Since the machine language program is represented in binary, it is very time-consuming for a human to write or understand.
However, the machine language is also just an abstraction of the computer's hardware and operating system.
Assembly provides enough abstraction to be comprehensible for a human while being a low-level representation of the program.
Some benefits of using assembly languages are increased runtime efficiency and decreased code size, while maintaining fine-grained control over the hardware and operating system.
However, achieving these results is often very demanding and time-consuming.
Due to the aforementioned reasons, it is often reasonable for a compiler to generate assembly code from the source program.
This way, the program is translated into a low-level, target-specific representation which allows the program to be executed on the target machine directly.
However, an assembler is still required in order to translate the assembly output of the compiler.
Since most assemblers output object files, a linker is required to create the final executable program.
Therefore, a compiler targeting the assembly of a specific architecture depends on these two additional steps before the program can be executed.

One might argue that the compiler could output object files directly.
However, doing so rarely creates any significant benefits other than the omitted dependence on the assembler.
Furthermore, implementing a compiler using this approach significantly increases the complexity of the compiler since it now has to perform the role of the assembler as well.

\subsection{Registers}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\begin{tikzpicture}[node distance=2cm]
		\node(CPU)[center] {CPU};
		\node(registers)[entity, left of=CPU, yshift=3cm] {Registers};
		\node(memory)[entity, right of=CPU, yshift=3cm] {Memory};

		% TODO: Maybe use `arrow` instead of `relation` in order to remove spacing
		\draw [darrow, very thick, double] (CPU) -- node[anchor=east, yshift=-.1cm] {fast} (registers);
		\draw [darrow] (CPU) -- node[anchor=west] {slow} (memory);
	\end{tikzpicture}
	\caption[Relationship between registers, memory, and the CPU.]{Relationship between registers, memory, and the CPU~\cite[pp.~20--21]{Dandamudi2005}.}\label{fig:cpu_reg_mem}
\end{wrapfigure}

Most processors contain numerous registers in order to hold data, instructions, and state information.
Unlike external memory, they are very limited in both size and count, and must therefore be used carefully.
Since they are a physical part of the CPU, they are much faster to access than the external memory.
Due to these traits, registers are very suitable for storing temporary values in larger computation~\cite[pp.~212--214]{Watson2017}.
Figure~\ref{fig:cpu_reg_mem} shows a simplified layout containing a CPU, registers, and memory.
Here, the CPU and its registers are connected via a thick arrow, indicating a high throughput connection.
However, the CPU and the memory are connected via a thin arrow which indicates a slower connection.
Nearly every CPU architecture will define an individual register layout,
meaning that sizes, count, and types of its registers may vary.
As a general rule, architectures with large number of registers usually outperform the ones that provide less registers.
However, as a result of this, the CPU will be more expensive to manufacture.
Since not every register is identical, there are platforms which distinguish between general-purpose, floating-point, and special-purpose registers.
For the majority of architectures, a subset of the registers is additionally designated for use in specific operations like \emph{procedure calls}~\cite[Chapter~2]{Dandamudi2005}.

\Lirsting[wrap=L, ranges={4-6}, wrap width=0.35\textwidth, caption={Example assembly program for explaining register allocation.}, label={lst:register_alloc}]{listings/register_alloc_simple.s}

For optimizing compilers, the limited number of registers presents a challenge as performance of the output program shall be maximized.
Some architectures may require that the operands of arithmetic or logical instructions have been explicitly loaded into registers beforehand.
In this case, registers would be required in nearly all computations.
In order to understand how register management can present a challenge, Listing~\ref{lst:register_alloc} should be considered.
It displays \riscv{} assembly instructions which calculate the sum of the two integers `40' and `2'.
In line~4, the integer value `40' is placed in the register \qVerb{a0}.
Next, in line~5, the integer `2` is placed in \qVerb{a1}.
In line~6, the \qVerb{add} instruction is used to calculate the sum of these two integers.
Since the first operand of the instruction specifies the register in which the result should be placed,
the original value of `40' in the register \qVerb{a0} would be overwritten by the instruction~\cite[reference]{Patterson2017}.
Through this example, it becomes apparent that instructions might use registers in a way that would interfere with other operations.
Therefore, subtle bugs can emerge as soon as an instruction unintentionally overwrites a register.

In compilers, the process of \emph{register allocation} is responsible for managing how registers are used.
This process attempts to use registers in a way that leads to maximized efficiency of the output program.
Since accessing registers is faster than accessing memory, a register allocator will try to use registers as often as possible.
Real compilers regularly follow this rule to the point at which normal variables are also kept in registers.
It is apparent that in most programs, the number of variables will certainly exceed the capacity provided by registers.
Therefore, register allocation has to detect when no free registers are available anymore.
In this case, the compiler has to save data in memory instead of registers.
This process is called \emph{register spilling}.
Since register spilling introduces a performance penalty, register allocators often attempt to use it as infrequently as possible.
Therefore, sophisticated algorithms are mandatory if execution performance is regarded as non-trivial.
Moreover, register allocation has to ensure that no conflicts between registers are introduced.
Such a conflict may be that a register is accidentally overwritten by an instruction in a completely unrelated block.
It is apparent that implementation of an algorithm performing all these tasks is usually very demanding.
For instance, register allocation often requires complex graph algorithms~\cite[pp.212-214]{Watson2017}.
Since register allocation represents a complex topic, it will not be explained any further.
However, there are also heavily simplified approaches to register allocation,
such as the ones used by the compilers presented in later sections of this chapter.

\subsection{Using Memory: The Stack}

As registers are limited in both size and count, additional storage, for example for variables is required.
For this, most architectures provide a \emph{stack}.
In general, a stack is a last-in-first-out data structure, meaning the element that was last added via a \emph{push} operation is the first to be removed by a \emph{pop} operation.
In computer hardware, the stack is usually physically separate from the CPU and therefore much slower to access.
It is typically implemented as linear memory that can be accessed and modified freely.
A so-called \emph{stack pointer}, which is normally stored in a special register, saves the address of the top-most element of the stack.
Thus, the principle of this memory being a stack data structure is merely a convention.
A stack typically grows towards lower addresses, so that in order to push values, the stack pointer must be decreased~\cite[pp.~68,99,100]{Patterson2017-zq}.

It is typical for compiled procedures to \emph{allocate} an entire region of the stack for themselves in a procedure's \emph{prologue} by subtracting the amount of needed bytes from the stack pointer, and freeing it in batch in a procedure's \emph{epilogue}.
These regions are called \emph{stack frames}, and some architectures, including both \riscv{} and x64, define an additional register for pointing to the other end of the current stack frame.
This pointer is usually called the \emph{frame pointer} or \emph{base pointer}~\cite[p.~94]{Waldron1998}.

\subsubsection{Alignment}

In order to save space, one might want to save variables of different sizes, say a boolean and an integer, directly next to each other on the stack.
However, most architectures require values to be \emph{aligned} to a multiple of their size.
Figure~\ref{fig:alignment} shows three example memory layouts of one \qVerb{u8}, one \qVerb{u16}, and one \qVerb{i64}\footnote{These types are defined by Rust, representing unsigned (\qVerb{u}) and signed (\qVerb{i}) integers with 8, 16, and 64 bits of storage respectively.}, taking up one, two, and eight bytes respectively.
The first layout has two of the integers misaligned, marked in dark gray, thus being invalid.
The second layout preserves the same order but inserts empty white areas, called \emph{padding}, in two places, to correct the alignment.
Firstly, there is one byte just before the \qVerb{u16} to align it to an integer multiple of two bytes, and secondly, there are four bytes before the \qVerb{i64} to have that be aligned to eight bytes.
The third layout inverts the order, which causes all three values to be correctly aligned as is.
\TODO{cite?}

\begin{figure}[htb]
	\centering

	\newcounter{aligncol}
	\setcounter{aligncol}{0}
	\newcounter{alignrow}
	\setcounter{alignrow}{0}
	\newcommand{\AlignedCell}[3][freecell]{%
		\draw[#1] (\value{aligncol},1.2*\value{alignrow}) +(0,-.5) rectangle +(#2,.5);
		\draw (\value{aligncol}+#2/2,1.2*\value{alignrow}) node {\strut #3};
		\addtocounter{aligncol}{#2}
	}
	\newcommand{\AlignNextRow}{\addtocounter{alignrow}{-1}\setcounter{aligncol}{0}}
	\begin{tikzpicture}[xscale=0.95, yscale=0.7]
		\footnotesize
		\AlignedCell{1}{\Verb{u8}}
		\AlignedCell[occupiedcell]{2}{\Verb{u16}}
		\AlignedCell[occupiedcell]{8}{\Verb{i64}}
		\AlignNextRow
		\AlignedCell{1}{\Verb{u8}}
		\AlignedCell[padding]{1}{}
		\AlignedCell{2}{\Verb{u16}}
		\AlignedCell[padding]{4}{}
		\AlignedCell{8}{\Verb{i64}}
		\AlignNextRow
		\AlignedCell{8}{\Verb{i64}}
		\AlignedCell{2}{\Verb{u16}}
		\AlignedCell{1}{\Verb{u8}}
	\end{tikzpicture}
	\caption{Examples of memory alignment.}\label{fig:alignment}
\end{figure}

Listing~\ref{lst:x64_align} shows the implementation of the rush x64 compiler for aligning a pointer to a given size.
The pointer must only be modified if it is not an integer multiple of the desired byte count yet, hence the outer if-expression.
To calculate the amount of padding, the formula \qVerbCmd{size - (ptr \% size)} is used.
For instance, say \qVerb{ptr} is `22' and \qVerb{size} is `8'.
The stack memory is then thought of in chunks of eight bytes each, where the resulting pointer should point to the start of the next empty chunk.
With the pointer currently being `22', there are two full chunks and one chunk with six of the eight bytes occupied.
Hence, two bytes of padding are needed to reach the next chunk.
The expression \qVerbCmd{22 \% 8} yields `6', which is the number of bytes that are already in use in the last chunk.
This result is then subtracted from `8' again to result in `2', the number of bytes required for padding that is then added to the pointer.

\Lirsting[ranges={184-188}, float=htb, caption={Alignment of values on the stack.}, label={lst:x64_align}]{deps/rush/crates/rush-compiler-x86-64/src/compiler.rs}

\subsection{Calling Conventions}

Programmers often use \emph{procedures} or \emph{functions} in order to structure their programs,
both to make the program easier to understand and to allow shared logic to be reused.
Procedures allow the programmer to focus on one portion of the tasks at the time,
thus representing one way of how a high-level programming language might provide abstraction.
Often, parameters and returned values act as the interface between the procedure and the remaining code.
During a procedure call in a high-level language like rush, many individual steps need to be performed in the program's low-level representation.
Since assembly does not support the use of high-level functions, most architectures specify their own \emph{calling convention} which describes how low-level function calls are to be performed.
On most architectures, a low-level procedure call follows these steps~\cite[p.~98]{Patterson2017-zq}:

\begin{enumerate}
	\item The caller places the call arguments in a place where the procedure can access them.
	\item Control is transferred to the procedure, often using a jump or specialized call instruction.
	      This specialized instruction often saves the \emph{return address}\footnote{Saves the address of the call-site,
		      allows the called procedure to return back to the caller~\cite[p.~99]{Patterson2017-zq}.} in a special register, so that function returns are possible.
	\item First, the procedure acquires local storage resources, like stack memory for its local variables.
	      This step is usually referred to as the \emph{prologue}.
	\item Internal code of the procedure is executed by evaluating the instructions in its body.
	\item The procedure's result value is placed somewhere so that the caller can access it.
	      Here resources allocated in step 3 are also released again.
	      This step is usually referred to as the \emph{epilogue}.
	\item Control is transferred back to the caller using a specialized return-instruction
	      that jumps back to the instruction which had initiated the procedure call.
\end{enumerate}

In order to leverage optimal performance during procedure calls, passed arguments are usually kept in registers.
However, in the previous subsection regarding registers, it was stated that the majority of architectures only plan for a portion of their registers to be used as call arguments.
If the number of call arguments exceeds the number of available argument registers, every remaining argument would have to be spilled to memory,
meaning that they are placed on the stack so that the called function can access them.~\cite[p.~98]{Patterson2017-zq}.

For instance, a target architecture might plan for \emph{four} registers (\qVerb{r0}--\qVerb{r3}) to be used as call arguments.
Now, a function is called using \emph{six} arguments.
Due to the fact that there are two more arguments than registers, register spilling is now required.
In that case, the registers \qVerb{r0}--\qVerb{r3} would contain the first four argument values while the fifth and sixth argument are spilled to the stack.
% Like hinted previously, most assembly procedures contain a \emph{prologue} and an \emph{epilogue}.
% These blocks of the function are responsible for allocating and deallocating resources which the function might use.
% For instance, the stack- and frame-pointer is often adjusted in the prologue.
% This way, space on the stack is allocated for variable declarations found in the called function.
% However, the modification offset always depends on how much memory the called function will require during runtime.
% Therefore, a compiler would have to keep track of the count of variable declarations made by a function.
% Apart from allocating stack space, the prologue is also often used for storing special registers on the stack.
%
% The epilogue however is required for deallocating all resources which were previously allocated by the function's prologue.
% For instance, the stack-related pointers are modified to reflect their state before the function call.
% Furthermore, any special registers previously saved on the stack are now restored.
% Obviously, a \qVerb{return} statement should always jump to the function's epilogue instead of terminating the function directly.
% At the end of most epilogues, a return-instruction jumping back to the caller location is often found.
% Therefore, the prologue is executed as the first code when calling the function and the epilogue is called as the last code before this function terminates.

It is to be mentioned that implementing a compiler which respects the calling convention of its target architecture is not required.
However, following the convention is often reasonable in order to preserve \emph{ABI}\footnote{Short for \enquote{application binary interface}, allows calling foreign functions from another program.} compatibility of the compiled program.

\subsection{Referencing Variables Using Pointers}\label{sec:pointers}

\Lirsting[wrap=o, fancyvrb={numbers=\OuterEdge}, wrap width=0.35\textwidth, caption={A rush program trying to alter the variable behind an argument.}, label={lst:rush_increment_by_value}]{listings/rush_increment_by_value.rush}

A \emph{pointer} is a variable which contains the memory address of another variable.
Leveraging pointers often leads to more efficient and compact code.
Furthermore, pointers are sometimes mandatory for solving a specific problem~\cite[p.~93]{Ritchie1988}.
A problem which can only be solved by using pointers is displayed in the rush program in Listing~\ref{lst:rush_increment_by_value}.
This program contains the \qVerb{main} and \qVerb{modify} functions.
In line~2, in the \qVerb{main} function, the mutable variable \qVerb{answer} is defined with an initial value of `42`.
Next, the \qVerb{modify} function is called, passing the variable as the call argument.
In line~4, the program exits with the value of \qVerb{answer}.
In the signature of the \qVerb{modify} function, the \qVerb{n} parameter is declared as a mutable integer.
Next, in line 8 of this function, the value of the parameter is incremented by `1`.
One might expect that the function call in line~3 causes the variable \qVerb{answer} to be incremented by `1`.
This seems likely since both the parameter and the variable are declared as mutable using the \qVerb{mut} keyword.
However, since rush passes function arguments by value and not by reference, the called function has no direct way of altering the variable behind the passed parameter\footnote{Passing by value means that the value of the argument is copied for the function call.}.
In order to solve this problem, pointers are required.
The code in Listing~\ref{lst:rush_increment_by_reference} displays a rush program solving this issue.

\Lirsting[wrap=o, fancyvrb={numbers=\OuterEdge}, wrap width=0.35\textwidth, caption={A rush program altering the variable behind an argument.}, label={lst:rush_increment_by_reference}]{listings/rush_increment_by_reference.rush}

Most parts of this rush program look similar to the one displayed in Listing~\ref{lst:rush_increment_by_value}.
However, some parts of the code have been adjusted so that they use a pointer.
First, the signature of the \qVerb{modify} function looks slightly different.
Now, the function takes a parameter of type \qVerb{*int} instead of \qVerb{int}.
The syntax \qVerb{*type} describes a pointer to the type specified after the \qVerb{*}.
Thus, the function now takes a parameter which is a pointer to an integer variable.
In rush, pointers allow full read-write access to the target of the variable.
This time, the parameter itself is not declared as mutable since the target variable behind the pointer and not the pointer itself should be incremented.
In line 8, the variable stored in the pointer is incremented.
When assigning to the target variable behind a pointer, the pointer first needs to be \emph{dereferenced}.
In rush, dereferencing a pointer is accomplished using the \qVerb{*} operator before the pointer's identifier.
The process of dereferencing a pointer involves accessing the value of the variable the pointer points to~\cite[p.~94]{Ritchie1988}.

In rush, only pointers targeting an existing variable can be created.
In order to create a pointer to a variable, rush's \qVerb{&} prefix operator is used.
This operator \emph{references} the target variable in order to return a pointer value.
Referencing a variable produces the memory address of that variable~\cite[p.~95]{Ritchie1988}.
Since the resulting value is only a normal integer, it could also be treated similarly.

However, some target architectures of a compiler might make the implementation of pointers more difficult.
The statement in line 8 increments the value of the variable pointed to by \qVerb{n} by 1.
This time, \qVerb{answer} does change since only the address, which still points to the same mutable variable, was copied during the function call.
Due to this write-access, the analyzer only allows the referencing (\qVerb{&}) operator to be used on \emph{mutable} variables.
As seen in the updated program, there is no need for the resulting pointer to be mutable since it only stores a read-only address.

A special trait of pointers in rush is that they are able to introduce \emph{undefined behavior}\footnote{The runtime behavior of such scenarios is undefined and might vary per architecture} into a program.
However, this only occurs if the pointer is used as a return value of a function in which it references a local variable.
This problem occurs because the memory used by that function is freed after the function has executed, resulting in the pointer referencing uninitialized memory.
Therefore, pointers should be used with caution since the semantic analyzer cannot guarantee that they are used correctly\footnote{If additional constraints for pointers are introduced, the analyzer could also validate their usage.}.

