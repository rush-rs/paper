\newpage
\section{Stages Of Compilation}
In order to minimize intricacy and to maximize modularity, compilation often involves several individual steps.
Here, the output of step $s$ serves as the input for step $s + 1$.
However, partitioning the compiler into as many steps as possible is prone to cause inefficiencies during compilation.
Separating the process of compilation into individual steps was the predominant technique until about 1980.
Due to limited memory of the computers at the time, a single-step compiler could not be implemented.
Therefore, only the individual steps of the compiler would fit,
as each step occupied a considerate amount of machine memory.
These types of compilers are called \emph{multipass compilers}.
However, the output of each step would be serialized and written to disk, ready to be read by the next step.
It is obvious that this partitioning leads to a lot of performance overhead, since disk access if often significantly slower than memory access.
Nowadays, we can mitigate these performance issues by implementing the compiler as a single program.
Therefore, the compiler can avoid slow disk operations by keeping intermediate structures solely in memory.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[node distance=2cm]
		\node (lexical_analysis) [rec] {lexical analysis};
		\node (syntactic_analysis) [rec, right of=lexical_analysis, xshift=2.5cm] {syntactical analysis};
		\draw [arrow] (lexical_analysis) -- node[anchor=south] {} (syntactic_analysis);
		\node (semantic_analysis) [rec, right of=syntactic_analysis, xshift=2.5cm] {semantic analysis};
		\draw [arrow] (syntactic_analysis) -- node[anchor=south] {} (semantic_analysis);
		\node (codegen) [rec, right of=semantic_analysis, xshift=2.5cm] {code generation};
		\draw [arrow] (semantic_analysis) -- node[anchor=south] {} (codegen);
	\end{tikzpicture}
	\caption{Different Steps Of Compilation}
	\label{fig:compilation-steps}
\end{figure}

\begin{enumerate}
	\item The lexical analysis (\emph{lexing}) translates sets of characters of the source program
	      into their corresponding symbols (\emph{tokens}) of the language's vocabulary.
	      For example, identifiers, operators, and delimiters are recognized
	      by examining each character of the source program in sequential order.
	\item The syntactical analysis (\emph{parsing}) transforms the previously generated sequence of tokens
	      into a data structure which directly represents the structure of the source program.
	\item The semantic analysis is responsible for validating
	      that the source program follows the semantic rules of the language.
	      Furthermore, this step often generates a new, similar data structure which contains additional type annotation and early optimizations.
	\item Code generation traverses the data structure generated by step three
	      in order to generate a sequence of target-machine instructions.
	      Due to likely constraints considering the target instruction set,
	      the code generation is often considered to be the most involved step of compilation.
\end{enumerate}
\cite[pp.~6--7]{wirth_compiler_construction_2005}
