In the previous chapter, we have learned how compilation to high-level targets can present an easy way of implementing a compiler.
These compilers generated outputs which were still platform independent and portable.
However, compilers can also target a specific computer architecture directly, thus removing another layer of abstraction.
The concept of compiling to a specific architecture directly is similar to the VM since its compiler also targets its architecture directly.
However, implementing a compiler targeting the architectures presented in this chapter has proven to be a lot more demanding since the VM uses a fictional architecture purposefully developed for this paper.
Reasons for this chapter's difficulty mostly include target-specific constraints which were
still irrelevant in the previous chapter.
This chapter contains the term \enquote{low-level} in its name since the presented compilers
generate code for specific target-architectures directly.

\section{Low-Level Programming Concepts}
Programming using high-level languages does not require knowledge about the target architecture of the program.
However, in this chapter, two compilers targeting the low-level assembly language are presented.
In order to make sections in which these compilers are explained more approachable,
some of the most important low-level programming concepts are explained in this section.
We will only explain concepts which play a significant role later on.

\subsection{Sections of an ELF File}
Since a program needs to be representable in a low-level manner, special formats are often required.
ELF stands for \enquote{executable and linkable format} and is often found on unix-like systems like Linux.
Programs using the ELF format can be represented in three different types of files.
For instance, object files generated by a compiler, like in the previous LLVM section, might use the ELF format.
Furthermore, libraries using \emph{shared object files} might also leverage the ELF format.
Most executable program use the format in order to represent a structured container for instructions, data, and additional information.
This way, the unit is mostly self-contained and can be executed by the operating system easily.
Therefore, ELF describes the format of a class of files and not just of an individual type of file~\cite[pp.~74-76]{Zhirkov2017-wk}.

Even though a processor only has access to one physical memory unit for both instructions and program data,
most assembly programs like to separate these types of memory into their separate components.
Therefore, an object file and assembly program is divided into so-called \emph{sections}~\cite[p.~19]{Zhirkov2017-wk}.
Some of the important ELF sections are displayed below~\cite[p.~76]{Zhirkov2017-wk}:

\begin{itemize}
	\item \qVerb{.text} stores the logic of the program represented using CPU instructions
	\item \qVerb{.rodata} stores read-only global data, it is often used for global constants.
	\item \qVerb{.data} stores mutable global data, such as mutable global variables
\end{itemize}

Even though a typical ELF file also contains other sections,
this list only includes entries which are of importance later in later sections of the paper.

\subsection{Assemblers and Assembly Language}

\begin{wrapfigure}{R}{.27\textwidth}
	\centering
	\begin{tikzpicture}[node distance=1cm, inner sep=3mm]
		\node (highlevel) [entity, fill=none] {High-level languages\\(\emph{C}, \emph{Rust}, \emph{rush})};
		\node (asm) [entity, below=of highlevel, align=center] {Assembly language\\level};
		\draw [arrow] (highlevel) -- (asm);
		\node (machine) [entity, fill=none, below=of asm, align=center] {Machine language\\level};
		\draw [arrow] (asm) -- (machine);
		\node (codegen) [entity, fill=none, below=of machine] {Operating system \\ and Hardware level};
		\draw [arrow] (machine) -- (codegen);
	\end{tikzpicture}
	\caption{Level of abstraction provided by assembly.}\label{fig:abstractions}
\end{wrapfigure}

Assembly language describes a type of low-level programming languages which are directly influenced by the target architecture.
Since the assembly code provides a slight abstraction over the computer's hardware,
the assembly code must be translated to machine code before it can be executed.
This process is performed by a program named the \emph{assembler} and is often called \emph{assembly}.
A typical characteristic of assembly code is that it seems relatively cryptic to a human reading it.
Furthermore, compared to high-level languages like C, assembly code is relatively low-level since it can be used to interact with the hardware directly.

For instance, the \riscv{} instruction \qVerb{add a0, a0, 2} would be used in integer addition.
This example contains most characteristics of an assembly language program.
Like hinted previously, the name of the instruction is a mnemonic.
In this case, \qVerb{addi} stands for \enquote{add immediate}.
Furthermore, the exact semantic meaning of the instruction is not immediately apparent.
At last, the instruction for adding integers differs for most CPU architectures.
For instance, an equivalent instruction for the x\_86 architecture could be `\texttt{add \%rdi, 2}'.
Therefore, the fact that instructions differ on each target architecture is clearly apparent.

As seen above, the application code in form of instructions is placed in the \qVerb{.text} section of an ELF binary.
In most assembly dialects, the programmer is able to partition the code manually using sections.
If the assembly code is assembled to an ELF object file, the \qVerb{.text} section of the assembly should contain all the instructions.
Just like parts of the LLVM IR, the \qVerb{.text} section of an assembly program obeys a hierarchy.
This section contains many labels which mark the beginning of a new basic block.
However, these basic blocks do not come with all the constraints which are to be followed when using LLVM IR\@.
For instance, a block in assembly might even contain no instructions, does not have to be terminated and could even be terminated twice.
Here, terminating instructions mean jump- and return-instructions.
Therefore, the constraints in assembly are much weaker than the ones introduced by LLVM\@.
Because an assembly usually omits complex optimizations, struct rules constraining the assembly code can be omitted too.

Since assembly provides significantly less abstraction than high-level languages, the question of how much abstraction is lost emerges.
In order to understand how much abstraction is provided by assembly, we should consider Figure~\ref{fig:abstractions}.
Here, the highest level of abstraction is provided by high-level programming languages like C or Rust.
In the context of this paper, rush presents roughly the same level of abstraction like these languages.
The next lower level of abstraction is provided by assembly.
Now, the program is no longer independent of its target architecture and is much more demanding to furmulate.
However, the next lower level of abstraction below assembly is represented by code in machine language.
As of today, one only rarely encounters a programmer writing programs using this level of abstraction.
Since the machine language program is represented in binary, it is nearly impossible for a human to write or understand.
However, the machine language is also just an abstraction of the computer's hardware and operating system.
Therefore, assembly provides enough abstraction to be comprehensible for a human while being a low-level representation of the program.
Although assembly provides more abstraction than the two levels at the bottom of the figure, programmers rarely program in assembly directly.
Some benefits of using assembly language to formulate a program are increased runtime efficiency and decreased code size while having fine-grained control over the hardware and operating system.
Therefore, it might be reasonable for a compiler to generate assembly code from the source program.
This way, the program is translated into a low-level, target-specific representation which allows the program to be executed on the target machine directly.
However, an assembler is still required in order to translate the assembly output of the compiler.
Since most assemblers output object files, a linker is required to create the final executable program.
Therefore, a compiler targeting the assembly of a specific architecture depends on these two additional steps before the program can be executed~\cite[p.~5-6]{Dandamudi2005}.

One might argue that the compiler could output object files directly.
However, doing so rarely creates any significant benefits other than the omitted dependence on the assembler.
Furthermore, implementing a compiler using this approach often significantly increases the complexity of the compiler since it now has to perform the role of the assembler as well.
However, the compiler could also emit binary data directly, thus making its implementation significantly more demanding.

\subsection{Registers}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\begin{tikzpicture}[node distance=2cm]
		\node(CPU)[center] {CPU};
		\node(registers)[entity, left of=CPU, yshift=3cm, vstack=4, rectangle split part align=left] {
			Registers
			\nodepart{two}{\texttt{r0}}
			\nodepart{three}{\texttt{r1}}
			\nodepart{four}{\ldots}
		};
		\node(memory)[entity, right of=CPU, yshift=3cm] {Memory};

		% TODO: Maybe use `arrow` instead of `relation` in order to remove spacing
		\draw [darrow, very thick, double] (CPU) -- node[anchor=east, yshift=-.1cm] {fast} (registers);
		\draw [darrow] (CPU) -- node[anchor=west] {slow} (memory);
	\end{tikzpicture}
	\caption{Relationship between registers, memory, and the CPU.}\label{fig:cpu_reg_mem}
\end{wrapfigure}

Most processors contain numerous registers in order to hold data, instructions, and state information.
In a computer, registers are sometimes used to hold data stored in variables.
However, most of the time, registers are used as temporary storage in large computations.
Registers are used in the latter case because they are way faster than memory.
Therefore, an algorithm using registers instead of memory will usually run faster.
However, even though registers are faster than memory, they are not used all the time.
This is because CPU registers are a limited resource, meaning every register-based CPU only has a finite amount of registers available.
Registers should therefore be used carefully and only when they are really needed~\cite[pp.~212-214]{Watson2017}.

Figure~\ref{fig:cpu_reg_mem} shows how the CPU interacts with its registers and the computer's memory.
The connection between the set of registers and the CPU is marked as a short and thick arrow because the connection between the CPU and its registers is very short and fast.
Although the registers are often physically parts of the CPU, they are displayed as separate entities in this example.
For the memory, the arrow is long and thin.
This represents the long and therefore slow connection between the CPU and the memory modules.
In most modern computers, this connection often spans across several centimeters on the motherboard.
Therefore, higher latency during memory access is inevitable~\cite[pp.~20-21]{Dandamudi2005}.

Nearly every CPU architecture will include an individual register layout, differing in size, count, and type.
Therefore, the exact information about registers always depends on which CPU is used.
As a rule of thumb, having more registers in an architecture will almost always improve performance of that architecture.
Furthermore, not every register is identical.
There might be platforms with some general-purpose, some floating-point, and some special-purpose registers.
For instance, there may be registers which contain information about the CPU's current instruction.
Furthermore, common architectures also include special-purpose registers for modifying the machine's memory.
Therefore, there is nearly always an appropriate register matching the requirement~\cite[Chapter~2]{Dandamudi2005}.

\Lirsting[wrap=L, ranges={4-6}, wrap width=0.3\textwidth, caption={Example assembly program for explaining register allocation.}, label={lst:register_alloc}]{listings/register_alloc_simple.s}

For a compiler, the limited amount of registers presents a big challenge.
Some architectures may require that the operands of arithmetic or logical instructions have been explicitly loaded into registers beforehand.
In this case, registers would be required in nearly all computations.
In order to understand how registers management can present a challenge, Listing~\ref{lst:register_alloc} should be considered.
This snippet displays \riscv{} assembly instructions which calculate the sum of the two integers 40 and 2.
In line 4, the integer value 40 is placed in the register \qVerb{a0}.
In line 5, another integer, this time 2 is placed in \qVerb{a1}.

Next, the \qVerb{add} instruction is used to calculate the sum of these two integers.
Since the first operand of the instruction specifies the register in which the result should be placed,
the original value of 40 in the register \qVerb{a0} would be overwritten by the instruction~\cite[reference]{Patterson2017}.
Through this example, it becomes apparent that other instructions might use registers unexpectedly.
Therefore, subtle bugs can be created if an instruction overwrites registers.

In compilers, the process of \emph{register allocation} is responsible for managing how registers are used.
This process attempts to use registers in a way leading to maximized efficiency of the output program.
Since accessing registers is often faster, a register allocator will try to use registers as often as possible.
Production-ready compilers often try to keep as much of the frequently accessed data in registers.
For instance, this frequently accessed data may also include variables which are normally saved in memory.
It is apparent that in most programs, the number of variables will certainly exceed the capacity provided by registers.
Therefore, register allocation has to detect when no free registers are available anymore.
In this case, the compiler has to save data in memory instead of registers.
This process of saving excess data in memory instead of registers is called \emph{register spilling}.
Since register spilling introduces a performance penalty, register allocation algorithms often attempt to prevent it as much as possible.
Therefore, sophisticated algorithms for register allocation are often mandatory as long as the factor of output code performance is non-trivial.

Apart from just managing the use of registers, most allocation algorithms are responsible for many other register-related tasks.
For instance, register allocation should detect when a variable is no longer needed so that its register can be freed.
Moreover, register allocation has to ensure that no conflicts between registers are introduced.
Such a conflict may be that a register is accidentally overwritten by an instruction in a completely unrelated basic block.
It is apparent that an algorithm performing all these tasks can not be implemented in an ad-hoc manner.
Instead, this process often requires complex graph algorithms for determining which registers can be used and freed.
Therefore, implementing register allocation in a compiler is often a very demanding task~\cite[pp.212-214]{Watson2017}.
Since register allocation represents a complex topic, it will not be explained any further.

\subsection{Using Memory: The Stack and the Heap}

As registers are limited in both size and count, additional storage for long-term memory is required.
This is what the \emph{stack} is for.
A stack in general is a last-in-first-out data structure, meaning the element that was last added via a \emph{push} operation is the first to be removed by a \emph{pop} operation.
In computer hardware the stack is usually separate from the CPU and therefore much slower to access.
It is usually implemented as a linear memory that can be freely accessed and modified, where a \emph{stack pointer}, which is usually stored in a special register, saves the address of the `top' of the stack.
Thus, the principal of this memory being a stack data structure is merely a convention.
A stack typically grows towards lower addresses, so that to push values the stack pointer must be decreased~\cite[pp.~68,99,100]{Patterson2017-zq}.
\TODO{ugly line wrap}

It is typical for compiled procedures to \emph{allocate} and entire region of the stack for themselves in a procedure's \emph{prologue} by subtracting the amount of needed bytes from the stack pointer, and freeing it in batch in a procedure's \emph{epilogue}.
These regions are called \emph{stack frames}, and some architectures, including both \riscv{} and x64, define an additional register for pointing to the other end of a stack frame.
This pointer is usually called \emph{frame pointer} or \emph{base pointer}~\cite[p.~94]{Waldron1998}.

\subsubsection{Alignment}

In order to save space, one might want to save variables of different sizes, say a boolean and an integer, directly next to each other on the stack.
However, most architectures require values to be aligned to a multiple of their size.
Figure~\ref{fig:alignment} shows three example memory layouts of one \qVerb{u8}, one \qVerb{u16}, and one \qVerb{i64}, taking up one, two, and eight bytes respectively.
\TODO{do we need to explain these Rust types?}
The first layout has two of the integers not correctly aligned, marked in dark gray, thus being invalid.
The second layout preserves the same order but inserts empty white areas in two places, called \emph{padding}, to correct the alignment.
First one byte just before the \qVerb{u16} to align it to an integral multiple of two bytes, and secondly four bytes before the \qVerb{i64} to have that be aligned to eight bytes.
The third layout inverts the order, which causes all three values to be correctly aligned as is.
\TODO{cite?}

\begin{figure}[htb]
	\centering

	\newcounter{aligncol}
	\setcounter{aligncol}{0}
	\newcounter{alignrow}
	\setcounter{alignrow}{0}
	\newcommand{\AlignedCell}[3][freecell]{%
		\draw[#1] (\value{aligncol},1.2*\value{alignrow}) +(0,-.5) rectangle +(#2,.5);
		\draw (\value{aligncol}+#2/2,1.2*\value{alignrow}) node {\strut #3};
		\addtocounter{aligncol}{#2}
	}
	\newcommand{\AlignNextRow}{\addtocounter{alignrow}{-1}\setcounter{aligncol}{0}}
	\begin{tikzpicture}[xscale=0.95, yscale=0.7]
		\footnotesize
		\AlignedCell{1}{\Verb{u8}}
		\AlignedCell[occupiedcell]{2}{\Verb{u16}}
		\AlignedCell[occupiedcell]{8}{\Verb{i64}}
		\AlignNextRow
		\AlignedCell{1}{\Verb{u8}}
		\AlignedCell[padding]{1}{}
		\AlignedCell{2}{\Verb{u16}}
		\AlignedCell[padding]{4}{}
		\AlignedCell{8}{\Verb{i64}}
		\AlignNextRow
		\AlignedCell{8}{\Verb{i64}}
		\AlignedCell{2}{\Verb{u16}}
		\AlignedCell{1}{\Verb{u8}}
	\end{tikzpicture}
	\caption{Examples of memory alignment.}\label{fig:alignment}
\end{figure}

Listing~\ref{lst:x64_align} shows the implementation of the rush x64 compiler for aligning a pointer to a given size.
The pointer must only be modified if it is not an integral multiple of the wanted byte count yet, hence the outer if-expression.
To calculate the amount of padding, the formula \qVerbCmd{size - (ptr \% size)} is used.
For instance, say \qVerb{ptr} is `22' and \qVerb{size} is `8'.
The stack memory is then thought of in chunks of eight bytes each, where the resulting pointer should point to the start of the next empty chunk.
With the pointer currently being `22', there are two full chunks and one chunk with six of the eight bytes occupied.
Hence, two bytes of padding are needed to reach the next chunk.
The expression \qVerbCmd{22 \% 8} yields `6', which is the number of bytes that are already in use in the last chunk.
This result is then subtracted from `8' again to result in `2', the number of bytes for padding that is then added to the pointer.

\Lirsting[ranges={184-188}, float=htb, caption={Alignment of values on the stack.}, label={lst:x64_align}]{deps/rush/crates/rush-compiler-x86-64/src/compiler.rs}

\TODO{some figure of a stack?}

\TODO{example for array?}

\TODO{The section title contains `Heap'. Can we just leave that out?}

\subsection{Calling Conventions}

Programmers often use \emph{procedures} or \emph{functions} in order to structure their programs,
both to make the program easier to understand and to allow shared logic to be reused.
Therefore, procedures allow the programmer to focus on one portion of the tasks at the time.
Often, parameters and returned values act as the interface between the procedure and the remaining code.
Therefore, procedures present one way of how a high-level programming language might provide abstraction.
During a procedure call in a high-level language like rush, many individual steps need to be performed in the program's low-level representation.
Since assembly does not support the use of high-level functions, most architectures specify their own \emph{calling convention} which describes how low-level function calls are to be performed.
On most architectures, a low-level procedure call might take place like the following steps:

\begin{itemize}
	\item The caller places the call arguments in a place where the procedure can access them.
	\item Control is transferred to the procedure, often using a jump or specialized call instruction.
	      This specialized instruction often saves the \emph{return address}\footnote{A link to the caller site, allows the called procedure to jump back to the caller~\cite[p.~99]{Patterson2017-zq}.} in a special register so that function returns are easier.
	\item The first task the called function performs is acquiring local storage resources needed by the procedure.
	      Often, registers are spilled if required.
	      Furthermore, stack memory for the procedure's local variables is often allocated in this step.
	\item Internal code of the procedure is run by executing the function body's instructions.
	\item The procedure's result value is placed somewhere so that the caller can access it.
	      Here resources allocated in step 3 are also released again.
	\item Control is transferred back to the caller using a specialized return-instruction.
	      A specialized instruction is often required since procedures can be called from multiple places in the program.
	      Therefore, the return-instruction jumps back to the instruction which had initiated the function call.
\end{itemize}

In order to leverage optimal performance during function calls, passed arguments are usually kept in registers.
However, in the previous subsection about registers, we have learned that most architectures state that only subset of their registers are to be used as function arguments.
Therefore, if a function is called using more than eight arguments, every remaining argument has to be spilled to memory.
In this case, each additional parameter of the function is placed inside the stack frame of the called function so that it can access it.~\cite[p.~98]{Patterson2017-zq}.

For instance, a target architecture might provide \emph{four} registers which can hold function arguments.
Now, a function is called using \emph{six} arguments.
Here, register spilling is mandatory since there are two more arguments than registers.
In that case, the registers \qVerb{r0} â€” \qVerb{r3} would contain the first four argument values while the fifth and sixth argument is spilled on the stack.

For managing the calling convention in assembly, most functions contain a \emph{prologue} and an \emph{epilogue}.
These blocks of the function are responsible for allocating and deallocating resources which the function might use.
For instance, the stack- and frame-pointer is often adjusted in the prologue.
This way, space on the stack is allocated for variable declarations found in the called function.
However, the modification offset always depends on how much memory the called function will require during runtime.
Therefore, a compiler would have to keep track of the count of variable declarations made by a function.
Apart from allocating stack space, the prologue is also often used for storing special registers on the stack.

The epilogue however is required for deallocating all resources which were previously allocated by the function's prologue.
For instance, the stack-related pointers are modified to reflect their state before the function call.
Furthermore, any special registers previously saved on the stack are now restored.
Obviously, a \qVerb{return} statement should always jump to the function's epilogue instead of terminating the function directly.
At the end of most epilogues, a return-instruction jumping back to the caller location is often found.
Therefore, the prologue is executed as the first code when calling the function and the epilogue is called as the last code before this function terminates.

It is to be mentioned that implementing a compiler which respects the calling convention of its target architecture is not required.
However, following the convention is often reasonable in order to preserve \emph{ABI}\footnote{Short for \enquote{application binary interface}, allows calling foreign functions from another program} compatibility of the compiled program.

\subsection{Referencing Variables Using Pointers}\label{sec:pointers}

A \emph{pointer} is a variable which contains the memory address of another variable.
Sometimes, pointers are mandatory for solving a specific computational problem.
Furthermore, leveraging pointers often leads to more efficient and compact code~\cite[p.~93]{Ritchie1988}.
A problem which can only be solved by using pointers is displayed in the rush program in Listing~\ref{lst:rush_increment_by_value}.

\Lirsting[float=H, caption={A rush program trying to alter the variable behind an argument.}, label={lst:rush_increment_by_value}]{listings/rush_increment_by_value.rush}

This rush program contains the \qVerb{main} and \qVerb{modify} functions.
In line 2 of the \qVerb{main} function, the mutable variable \qVerb{answer} is defined with an initial value of 42.
Next, the \qVerb{modify} function is called, passing the variable as the call argument.
In line 4, the program exits, using the value of \qVerb{answer} as its exit code.
In the signature of the \qVerb{modify} function, the \qVerb{n} parameter is declared as a mutable integer.
Next, in line 8 of this function, the value of the parameter is incremented by 1.
One might expect that the function call in line 3 causes the variable \qVerb{answer} to be incremented by 1.
This seems likely since both the parameter and the variable are declared as mutable using the \qVerb{mut} keyword.
However, since rush passes function arguments by value and not by reference, the called function has no direct way of altering the variable behind the passed parameter
\footnote{Passing by value means that the value of the argument is copied for the function call}.
In order to solve this problem, pointers are required.
The code in Listing~\ref{lst:rush_increment_by_reference} displays a rush program which is able to solve this problem.

\Lirsting[float=H, caption={A rush program altering the variable behind an argument.}, label={lst:rush_increment_by_reference}]{listings/rush_increment_by_reference.rush}

Most parts of this rush program look similar to the one displayed in Listing~\ref{lst:rush_increment_by_value}.
However, some parts of the code have been adjusted so that they use pointers.
First, the signature of the \qVerb{modify} function looks slightly different.
Now, the function takes a parameter of type \qVerb{*int} instead of \qVerb{int}.
The syntax \qVerb{*type} describes a pointer to the type specified after the \qVerb{*}.
Thus, the function now takes a parameter which is a pointer to an integer variable.
In rush, pointers allow full read-write access to the target of the variable.
The parameter is not declared as mutable since the target variable behind the pointer and not the pointer itself should be incremented.
In line 8, the variable stored in the pointer is incremented.
When assigning to the target variable behind a pointer, the pointer first needs to be dereferenced using the \qVerb{*} prefix operator.

In rush, dereferencing a pointer is accomplished using the \qVerb{*} operator before the pointer's identifier.
The process of \emph{dereferencing} a pointer involves accessing the value of the variable the pointer points to~\cite[p.~94]{Ritchie1988}.
For instance, a pointer variable named \qVerb{p} would be dereferenced by using the following rush expression: `\LirstInline{rush}{*p}'.

In rush, only pointers targeting an existing variable can be created.
In order to create a pointer to an existing variable, rush's \qVerb{&} prefix operator is used.
This operator \emph{references} the target variable in order to return a pointer value.
\emph{Referencing} a variable produces the memory address of that variable, often in form of an integer value~\cite[p.~95]{Ritchie1988}.
Theoretically, since the resulting value is only a normal integer, it can also be treated similarly.
Therefore, creating a new pointer variable involves following steps:

\begin{enumerate}
	\item Producing the absolute memory address of a variable
	\item Saving the resulting integer as a variable on the stack as if it was a conventional integer
\end{enumerate}

It is apparent that implementing pointers should often be relatively straight-forward since the underlying principle consists of only these two simple steps.
However, some target architectures of a compiler might make the implementation of pointers more difficult.
Therefore, the statement in line 8 increments the value of the variable stored in \qVerb{n} by 1.
Here, only the value of the pointer is copied when being used as an argument.
Since the pointer only saves the address of its target, the copy does not affect the target variable by any means.
Because rush pointers allow write-access, \qVerb{answer} can be incremented by using a pointer.

Due to this write-access, the analyzer only allows the referencing (\qVerb{&}) operator to be used on mutable variables.
As seen in the updated program, there is no need for the resulting pointer to be mutable since it only stores a read-only address.

A special trait of pointers in rush is that they are able to introduce \emph{undefined behavior}\footnote{The runtime behavior of such scenarios is undefined and might vary on every architecture} into a program.
This however only occurs if the pointer is used as a return value of a function in which it references a local variable.
This problem occurs because the memory used by that function is often freed after the function has executed.
Therefore, pointers should be used with caution since the semantic analyzer cannot guarantee that they are used correctly.

