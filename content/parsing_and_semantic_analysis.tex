\chapter{Syntactical And Semantic Analysis}

\section*{Introduction}
\textcolor{red}{Should this heading exist?}

Programs are often formulated in complex structures which represent the logic of the underlying algorithm.
However, a computer can only interpret a sequence of CPU instructions.
Therefore, the source program has to be translated into such a sequence of target-dependent instructions before it can be executed.
The translation process is called \emph{compilation}, and is performed by program which is called \emph{a compiler}.
However, the output instruction sequence must represent the identical algorithm specified in the source code.
It is apparent that compilation requires significant effort and must obey complex rules,
since it should translate the source program precisely.

The first compiler was implemented around 1956 and aimed to translate \emph{Fortran} to computer instructions.
However, the success of this programming endeavor was not assured until the program was completed.
In total, the program involved roughly 18 man years of work
and is thereby regarded as one of the largest programming projects of the time.

\section{Different Stages Of Compilation}
In order to minimize intricacy and to maximize modularity, compilation often involves several individual steps.
Here, the output of step $s$ serves as the input for step $s + 1$.
However, partitioning the compiler into as many steps as possible is prone to cause inefficiencies during compilation.
Separating the process of compilation into individual steps was the predominant technique until about 1980.
Due to limited memory of the computers at the time, a single-step compiler could not be implemented.
Therefore, only the individual steps of the compiler would fit,
as each step occupied a considerate amount of machine memory.
However, the output of each step would be serialized and written to disk, ready to be read by the next step.
It is obvious that this partitioning leads to a lot of overhead which can be omitted by using a more modern approach.
\cite[pp.~6--7]{wirth_compiler_construction_2005}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[node distance=2cm]
		\node (lexical_analysis) [rec] {lexical analysis};
		\node (syntactic_analysis) [rec, right of=lexical_analysis, xshift=2.5cm] {syntactical analysis};
		\draw [arrow] (lexical_analysis) -- node[anchor=south] {} (syntactic_analysis);
		\node (semantic_analysis) [rec, right of=syntactic_analysis, xshift=2.5cm] {semantic analysis};
		\draw [arrow] (syntactic_analysis) -- node[anchor=south] {} (semantic_analysis);
		\node (codegen) [rec, right of=semantic_analysis, xshift=2.5cm] {code generation};
		\draw [arrow] (semantic_analysis) -- node[anchor=south] {} (codegen);
	\end{tikzpicture}
	\caption{Different Steps Of Compilation}
	\label{fig:compilation-steps}
\end{figure}

\setcounter{secnumdepth}{5}

\begin{enumerate}
	\item The lexical analysis (\emph{lexing}) translates sets of characters of the source program
	      into their corresponding symbols (\emph{tokens}) of the language's vocabulary.
	      For example, identifiers, operators, and delimiters are recognized
	      by examining each character of the source program in sequential order.
	\item The syntactical analysis (\emph{parsing}) transforms the previously generated sequence of tokens
	      into a data structure which directly represents the structure of the source program.
	\item The semantic analysis is responsible for validating
	      that the source program follows the semantic rules of the language.
	      Furthermore, this step often generates a new, similar data structure which contains additional type annotation and early optimizations.
	\item Code generation traverses the data structure generated by step three
	      in order to generate a sequence of target-machine instructions.
	      Due to likely constraints considering the target instruction set,
	      the code generation is often considered to be the most involved step of compilation.
\end{enumerate}

\textcolor{red}{TODO: continue work here}

\cite[p.~6]{wirth_compiler_construction_2005}

\section{Lexical And Syntactical Analysis}

\section{Semantic Analysis}

A programming language is not just defined by a grammar but often also by a semantic specification.
Unlike the grammar, the semantic specification does not specify the language's syntax.
Instead, this  specification often mandates how a program should behave.
Common rules include \emph{data types} and \emph{integer overflow behavior}.
Another example of a semantic rule is that a variable has to be defined before it is used.
Defining the semantic rules a the programming language is often a hard task because not all requirements are clear from the beginning.
Since the semantic rules of a programming language can not be defined in a formal way,
a language designer often writes their specification in a natural language, meaning Chomsky type 0.
However, due to the specification being written in a natural language, the specification can sometimes be ambiguous.
Since those rules define when a program is valid, they have to be checked and enforced before program compilation can start.

Because rush shares its semantic rules across all backends,
it would be cumbersome to implement semantic validation in each
backend individually. Therefore, it would be rational to implement a separate
compilation step which is responsible for validating the source program's
semantics. Among other checks, the so-called semantic analyzer validates types
and variable references whilst performing type annotations. The last aspect is
of particular importance since all compiler backends rely on type information at
compile time. However, in order to obtain type information, the abstract syntax
tree of the source program must be traversed, performing numerous other checks
in the process. The semantic analyzer behaves exactly like this. In order to
preserve a clear boundary between the individual compilation steps, the parser
only validates the program's syntax without performing further validation.
Therefore, the analyzer traverses the abstract syntax tree previously generated
by the parser.

In order to highlight why type information is often required at compile time, we
will consider following code example. The listing \ref{lst:analyzer-test} displays a basic rush program
calculating the sum of two integer values in order to use the sum as its
exit-code. In this example, the desired exit-code of the program is five.

\TSListing[caption={A rush Program Which Adds Two Integers}, label={lst:analyzer-test}, float=H]{semantic_analysis_simple.rush}

In this example, the semantic analyzer will first check if the program contains
a \texttt{main} function. If this is not the case, the analyzer rejects the program
because it violates rush's semantic specification. Just like C or Rust, rush
requires that every program must contain exactly one \texttt{main} function. If a
programming languages requires a \texttt{main} function, top-level statements can be
completely forbidden, allowing more elegant code imports between different
source files. Furthermore, a \texttt{main} function clearly indicates the program's
start of execution which removes ambiguity. Therefore, the analyzer checks that
the \texttt{main} function takes no parameters and returns no value. However, in this
example, there is a valid \texttt{main} function. Now, the analyzer traverses the
function body of the \texttt{main} function. Since \texttt{let} statements are used to declare
new variables, the analyzer will add the variables \texttt{two} and \texttt{three} to its
current scope. However, unlike an interpreter, the analyzer does not insert the
variable's value into its scope. Instead of the concrete value, the analyzer
only considers the types of expressions. Therefore, in this example, the
analyzer remembers that the variables \texttt{two} and \texttt{three} store integer values.
This information will become much more useful when we consider line 4. Here, the
analyzer checks that the identifiers \texttt{two} and \texttt{three} refer to valid variables.
Just like in most other programming languages, rush does not allow the addition
of two boolean values for example. Here, the analyzer checks that the operands
of the `+` operator have the same type and that this type is valid in additions.
Because this validation requires information about types, the analyzer accesses
its scope when looking up the identifiers \texttt{two} and \texttt{three}. Since those names
were previously mapped to the \texttt{int} type, the analyzer is now aware of the
operand types and can check their validity. In this case, calculating the
sum of two integers is legal and results in another integer value. Since rush's
semantic specification states that the \texttt{exit} function requires exactly one
integer parameter, the analyzer has to check that it is called correctly.
Furthermore, the analyzer validates all function calls and declarations, not
just the ones of builtin functions. Since the result of the addition is also an
integer, the analyzer accepts this program since both its syntax and semantics
are valid.

As indicated previously, most compilers require type information whilst
generating target code. For simplicity, we will consider a fictional compiler
which can compile both integer and float additions. However, the fictional
target machine requires different instructions for addition depending on the
type of the operands. Therefore, integer addition uses the \texttt{intadd} instruction
while float addition uses the \texttt{floatadd} instruction. Here, type ambiguity would
cause difficulties. If there was no semantic analysis step, the compiler would
have to implement its own way of determining the types of the operands at
compile time. However, determining these types requires a complete
tree-traversal of the operand expressions. Due to the recursive design of the
abstract syntax tree, implementing this tree-traversal would require a
significant amount of source code in the compiler. However, the implementation
of this algorithm would be nearly identical accross all of rush's compiler
backends. Therefore, implementing type determination in each backend
individually would enlarge the compiler source code, thus making it harder to
understand. Since code duplication is considered inelegant, outsourcing this
algorithm into a separate component is likely the best option. As a result of
this, the semantic analyzer implements such a tree-traversal algorithm for
determining the types of subtrees. Because of the previously mentioned reasons,
rush's semantic analyzer also annotates the abstract syntax tree with type
information so that it is usable for later steps of compilation.

In order to obtain a better understanding of how the analyzer works, we will now
consider parts of its implementation and how they behave when analyzing the
example from above.
